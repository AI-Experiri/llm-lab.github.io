[
	{
		"Name": "MultiLoKo",
		"Type": "language & reasoning,multilingual",
		"Description": "A new benchmark for evaluating multilinguality in LLMs covering 31 languages.",
		"Benchmark paper": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages \nhttps://arxiv.org/abs/2504.10356",
		"Code repository": "https://github.com/facebookresearch/multiloko",
		"Dataset ": "see repo",
		"Number of examples": "15500",
		"License": "MIT License",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "FACTS Grounding",
		"Type": "safety",
		"Description": "A measure of how accurately LLMs ground their responses in provided source material and avoid hallucinations.",
		"Benchmark paper": "FACTS Grounding: A new benchmark for evaluating the factuality of large language models\nPublished \nhttps://arxiv.org/abs/2501.03200",
		"Code repository": "https://www.kaggle.com/code/andrewmingwang/facts-grounding-benchmark-starter-code",
		"Dataset ": "https://www.kaggle.com/datasets/deepmind/facts-grounding-examples",
		"Number of examples": "1719",
		"License": "CC-BY-4.0",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "Graphwalks",
		"Type": "language & reasoning",
		"Description": "A dataset for evaluating multi-hop long-context reasoning. In Graphwalks, the model is given a graph represented by its edge list and asked to perform an operation.",
		"Benchmark paper": "Introducing GPT-4.1 in the API \nhttps://openai.com/index/gpt-4-1/",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/openai/graphwalks",
		"Number of examples": "1150",
		"License": "MIT License",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "NoLiMa",
		"Type": "information retrieval & RAG",
		"Description": "Extended NIAH, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack.",
		"Benchmark paper": "NoLiMa: Long-Context Evaluation Beyond Literal Matching \nhttps://arxiv.org/abs/2502.05167",
		"Code repository": "https://github.com/adobe-research/NoLiMa",
		"Dataset ": "https://huggingface.co/datasets/amodaresi/NoLiMa",
		"Number of examples": "7540",
		"License": "Adobe Research License",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "MultiChallenge",
		"Type": "conversation & chatbots",
		"Description": "Evaluates LLMs on conducting multi-turn conversations with human users across 4 challenges: instruction retention, inference memory, reliable versioned editing, and self-coherence. ",
		"Benchmark paper": "MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs \nhttps://arxiv.org/abs/2501.17399",
		"Code repository": "https://github.com/ekwinox117/multi-challenge",
		"Dataset ": "https://github.com/ekwinox117/multi-challenge/tree/main/data",
		"Number of examples": "273",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "ColBench",
		"Type": "agents & tools use",
		"Description": "A new benchmark, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design.",
		"Benchmark paper": "SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks \nhttps://arxiv.org/abs/2503.15478",
		"Code repository": "https://github.com/facebookresearch/sweet_rl",
		"Dataset ": "https://huggingface.co/datasets/facebook/collaborative_agent_bench",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "AlpacaEval",
		"Type": "instruction-following,conversation & chatbots",
		"Description": "An automatic evaluator for instruction-following LLMs.",
		"Benchmark paper": "Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators\nhttps://arxiv.org/abs/2404.04475",
		"Code repository": "https://github.com/tatsu-lab/alpaca_eval",
		"Dataset ": "https://huggingface.co/datasets/tatsu-lab/alpaca_eval",
		"Number of examples": "n/a",
		"License": "CC-BY-NC-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "MT-Bench-101",
		"Type": "conversation & chatbots",
		"Description": "Multi-turn dialogues.",
		"Benchmark paper": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues\nhttps://arxiv.org/abs/2402.14762 ",
		"Code repository": "https://github.com/mtbench101/mt-bench-101",
		"Dataset ": "https://github.com/mtbench101/mt-bench-101/tree/main/data/subjective",
		"Number of examples": "4208",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Chatbot Arena",
		"Type": "conversation & chatbots",
		"Description": "Open-source platform for comparing LLMs in a competitive environment.",
		"Benchmark paper": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference\nhttps://arxiv.org/abs/2403.04132\n",
		"Code repository": "https://github.com/lm-sys/FastChat/tree/main",
		"Dataset ": "https://huggingface.co/datasets/lmsys/chatbot_arena_conversations",
		"Number of examples": "33000",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "HarmBench",
		"Type": "safety",
		"Description": "Adversarial behaviors including cybercrime, copyright violations, and generating misinformation (https://www.harmbench.org). ",
		"Benchmark paper": "https://arxiv.org/abs/2402.04249",
		"Code repository": "https://github.com/centerforaisafety/HarmBench/tree/main",
		"Dataset ": "https://github.com/centerforaisafety/HarmBench/tree/main/data/behavior_datasets",
		"Number of examples": "510",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "MMLU Pro",
		"Type": "knowledge,language & reasoning",
		"Description": "An enhanced dataset designed to extend the MMLU benchmark. More challenging questions, the choice set of ten options.",
		"Benchmark paper": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark\nhttps://arxiv.org/abs/2406.01574",
		"Code repository": "https://github.com/TIGER-AI-Lab/MMLU-Pro",
		"Dataset ": "https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro",
		"Number of examples": "12100",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "MixEval",
		"Type": "conversation & chatbots",
		"Description": "A ground-truth-based dynamic benchmark derived from off-the-shelf benchmark mixtures.",
		"Benchmark paper": "MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures\nhttps://arxiv.org/abs/2406.06565",
		"Code repository": "https://github.com/Psycoy/MixEval",
		"Dataset ": "https://huggingface.co/datasets/MixEval/MixEval",
		"Number of examples": "5000",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "SimpleQA",
		"Type": "safety",
		"Description": "Measures the ability for language models to answer short, fact-seeking questions to reduce hallucinations.",
		"Benchmark paper": "Measuring short-form factuality in large language models\nhttps://arxiv.org/abs/2411.04368",
		"Code repository": "https://github.com/openai/simple-evals",
		"Dataset ": "https://huggingface.co/datasets/basicv8vc/SimpleQA",
		"Number of examples": "4326",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "CRUXEval (Code Reasoning, Understanding, and Execution Evaluation)",
		"Type": "coding",
		"Description": "A set of Python functions and input-output pairs that consists of two tasks: input prediction and output prediction.",
		"Benchmark paper": "CRUXEval: A Benchmark for Code Reasoning,\nUnderstanding and Execution\nhttps://arxiv.org/abs/2401.03065",
		"Code repository": "https://github.com/facebookresearch/cruxeval",
		"Dataset ": "https://huggingface.co/datasets/cruxeval-org/cruxeval",
		"Number of examples": "800",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "AgentHarm",
		"Type": "safety",
		"Description": "Explicitly malicious agent tasks, including fraud, cybercrime, and harassment.",
		"Benchmark paper": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents\nhttps://arxiv.org/abs/2410.09024",
		"Code repository": "https://github.com/UKGovernmentBEIS/inspect_evals",
		"Dataset ": "https://huggingface.co/datasets/ai-safety-institute/AgentHarm",
		"Number of examples": "110",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "StrongReject",
		"Type": "safety",
		"Description": "Tests a model\u2019s resistance against common attacks from the literature.",
		"Benchmark paper": "A StrongREJECT for Empty Jailbreaks\nhttps://arxiv.org/abs/2402.10260",
		"Code repository": "https://github.com/dsbowen/strong_reject",
		"Dataset ": "https://github.com/dsbowen/strong_reject/tree/main/docs/api",
		"Number of examples": "n/a",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "BFCL (Berkeley Function-Calling Leaderboard)",
		"Type": "agents & tools use",
		"Description": "A set of function-calling tasks, including multiple and parallel function calls.",
		"Benchmark paper": "Berkeley Function-Calling Leaderboard\nhttps://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html",
		"Code repository": "https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard",
		"Dataset ": "https://huggingface.co/datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard",
		"Number of examples": "2000",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "TrustLLM",
		"Type": "safety,bias & ethics",
		"Description": "A benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. Consists of over 30 datasets.",
		"Benchmark paper": "TrustLLM: Trustworthiness in Large Language Models\nhttps://arxiv.org/abs/2401.05561",
		"Code repository": "https://github.com/HowieHwong/TrustLLM",
		"Dataset ": "https://github.com/HowieHwong/TrustLLM?tab=readme-ov-file#dataset-download",
		"Number of examples": "n/a",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "BigCodeBench",
		"Type": "coding",
		"Description": "Function-level code generation tasks with complex instructions and diverse function calls.",
		"Benchmark paper": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions\nhttps://arxiv.org/abs/2406.15877",
		"Code repository": "https://github.com/bigcode-project/bigcodebench",
		"Dataset ": "https://github.com/bigcode-project/bigcodebench",
		"Number of examples": "1140",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "AIR-Bench",
		"Type": "safety",
		"Description": "AI safety benchmark aligned with emerging regulations. Considers operational, content safety, legal and societal risks (https://crfm.stanford.edu/helm/air-bench/latest/).",
		"Benchmark paper": "https://arxiv.org/abs/2407.17436",
		"Code repository": "https://github.com/stanford-crfm/air-bench-2024",
		"Dataset ": "https://huggingface.co/datasets/stanford-crfm/air-bench-2024",
		"Number of examples": "5,694",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "WildChat",
		"Type": "conversation & chatbots",
		"Description": "A collection of 1 million conversations between human users and ChatGPT, alongside demographic data (https://wildchat.allen.ai/about). ",
		"Benchmark paper": "WildChat: 1M ChatGPT Interaction Logs in the Wild\nhttps://arxiv.org/abs/2405.01470",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/allenai/WildChat-1M",
		"Number of examples": "1,000,000+",
		"License": "ODC-BY license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Video-MME",
		"Type": "language & reasoning,multimodal,video",
		"Description": "A benchmark for multimodal long context understanding.",
		"Benchmark paper": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis \nhttps://arxiv.org/abs/2405.21075 ",
		"Code repository": "https://github.com/MME-Benchmarks/Video-MME",
		"Dataset ": "https://github.com/MME-Benchmarks/Video-MME?tab=readme-ov-file#-dataset",
		"Number of examples": "900",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "101-500"
	},
	{
		"Name": "OR-Bench",
		"Type": "safety",
		"Description": "80,000 benign prompts likely rejected by LLMs across 10 common rejection categories.",
		"Benchmark paper": "An Over-Refusal Benchmark for Large\nLanguage Models\nhttps://arxiv.org/abs/2405.20947 ",
		"Code repository": "https://github.com/justincui03/or-bench",
		"Dataset ": "https://huggingface.co/datasets/bench-llm/or-bench",
		"Number of examples": "80000",
		"License": "CC-BY-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "BiGGen-Bench",
		"Type": "language & reasoning,agents & tools use,safety,instruction-following",
		"Description": "Evaluates nine distinct capabilities of LMs, including instruction following, reasoning, tool usage, and safety.",
		"Benchmark paper": "The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models \nhttps://arxiv.org/abs/2406.05761",
		"Code repository": "https://github.com/prometheus-eval/prometheus-eval/tree/main/BiGGen-Bench",
		"Dataset ": "https://huggingface.co/datasets/prometheus-eval/BiGGen-Bench",
		"Number of examples": "765",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Global MMLU",
		"Type": "bias & ethics",
		"Description": "Translated MMLU, that also includes cultural sensitivity annotations for a subset of the questions, with evaluation coverage across 42 languages.",
		"Benchmark paper": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation \nhttps://arxiv.org/abs/2412.03304 ",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/CohereForAI/Global-MMLU",
		"Number of examples": "601734",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "MC-Bench (Minecraft AI Benchmark)",
		"Type": "image generation",
		"Description": "A platform for evaluating and comparing AI models by challenging them to create Minecraft builds.",
		"Benchmark paper": "https://mcbench.ai/",
		"Code repository": "https://github.com/mc-bench",
		"Dataset ": "Not dataset-based",
		"Number of examples": "n/a",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "TOFUEVAL",
		"Type": "safety",
		"Description": "Evaluation benchmark on topic-focused dialogue summarization. Contains binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences.",
		"Benchmark paper": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization\nhttps://arxiv.org/abs/2402.13249",
		"Code repository": "https://github.com/amazon-science/tofueval",
		"Dataset ": "see repo",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "BackdoorLLM",
		"Type": "safety",
		"Description": "A benchmark for backdoor attacks in text generation.",
		"Benchmark paper": "BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models \nhttps://arxiv.org/abs/2408.12798",
		"Code repository": "https://github.com/bboylyg/BackdoorLLM",
		"Dataset ": "https://huggingface.co/datasets/BackdoorLLM/Backdoored_Dataset",
		"Number of examples": "4200",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "AIME",
		"Type": "math",
		"Description": "This dataset contains problems from the American Invitational Mathematics Examination (AIME) 2024.",
		"Benchmark paper": "n/a",
		"Code repository": "https://artofproblemsolving.com/wiki/index.php/American_Invitational_Mathematics_Examination",
		"Dataset ": "https://huggingface.co/datasets/Maxwell-Jia/AIME_2024",
		"Number of examples": "30",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "RULER",
		"Type": "information retrieval & RAG",
		"Description": "A synthetic benchmark with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles.",
		"Benchmark paper": "RULER: What's the Real Context Size of Your Long-Context Language Models? \nhttps://arxiv.org/abs/2404.06654",
		"Code repository": "https://github.com/NVIDIA/RULER",
		"Dataset ": "see repo",
		"Number of examples": "13",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "ClinicBench",
		"Type": "domain-specific",
		"Description": "Datasets and clinical tasks that are common in real-world medical practice, e.g., open-ended decision-making, long document processing, and emerging drug analysis.",
		"Benchmark paper": "Large Language Models in the Clinic: A Comprehensive Benchmark \nhttps://arxiv.org/abs/2405.00716",
		"Code repository": "https://github.com/AI-in-Health/ClinicBench",
		"Dataset ": "https://github.com/AI-in-Health/ClinicBench",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Hal-eval",
		"Type": "safety",
		"Description": "Assesses LVLMs' ability to tackle a broad spectrum of hallucinations.",
		"Benchmark paper": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models \nhttps://arxiv.org/abs/2402.15721",
		"Code repository": "https://github.com/WisdomShell/hal-eval",
		"Dataset ": " https://github.com/WisdomShell/hal-eval/tree/main/evaluation_dataset",
		"Number of examples": "2000000",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Zebralogic",
		"Type": "language & reasoning",
		"Description": "Evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ",
		"Benchmark paper": "ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning \nhttps://arxiv.org/abs/2502.01100",
		"Code repository": "https://github.com/WildEval/ZeroEval",
		"Dataset ": "https://huggingface.co/datasets/WildEval/ZebraLogic",
		"Number of examples": "4259",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "JudgeBench",
		"Type": "llm judge evaluation",
		"Description": "A benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. Evaluated on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models.",
		"Benchmark paper": "JudgeBench: A Benchmark for Evaluating LLM-based Judges \nhttps://arxiv.org/abs/2410.12784 ",
		"Code repository": "https://github.com/ScalerLab/JudgeBench?tab=readme-ov-file",
		"Dataset ": "https://huggingface.co/datasets/ScalerLab/JudgeBench",
		"Number of examples": "620",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "FRAMES (Factuality, Retrieval, And reasoning MEasurement Set)",
		"Type": "information retrieval & RAG,language & reasoning,safety",
		"Description": "Tests the capabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy, and reasoning.",
		"Benchmark paper": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation \nhttps://arxiv.org/abs/2409.12941",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/google/frames-benchmark",
		"Number of examples": "824",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "NovelQA",
		"Type": "language & reasoning",
		"Description": "Tests deep textual understanding in LLMs with extended texts. Constructed from English novels.",
		"Benchmark paper": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens \nhttps://arxiv.org/abs/2403.12766",
		"Code repository": "https://github.com/NovelQA/novelqa.github.io",
		"Dataset ": "https://huggingface.co/datasets/NovelQA/NovelQA",
		"Number of examples": "2305",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "MMNeedle",
		"Type": "multimodal,information retrieval & RAG",
		"Description": "MultiModal Needle-in-a-haystack (MMNeedle) benchmark is designed to assess the long-context capabilities of MLLMs.",
		"Benchmark paper": "Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models \nhttps://arxiv.org/abs/2406.11230",
		"Code repository": "https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack",
		"Dataset ": "https://drive.google.com/drive/folders/1D2XHmj466e7WA4aY7zLkbdTmp3it2ZPy",
		"Number of examples": "880000",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "OmniEval",
		"Type": "information retrieval & RAG,language & reasoning,domain-specific",
		"Description": "RAG benchmark in the financial domain, with queries in five task classes and 16 financial topics.",
		"Benchmark paper": "OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain \nhttps://arxiv.org/abs/2412.13018",
		"Code repository": "https://github.com/RUC-NLPIR/OmniEval",
		"Dataset ": "see repo",
		"Number of examples": "N/A",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "CHAMP",
		"Type": "math",
		"Description": "High-school math problems, annotated with general math facts and problem-specific hints. These annotations allow exploring the effects of additional information, such as relevant hints, misleading concepts, or related problems. ",
		"Benchmark paper": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities \nhttps://arxiv.org/abs/2401.06961",
		"Code repository": "https://github.com/YilunZhou/champ-dataset",
		"Dataset ": "https://yujunmao1.github.io/CHAMP/explorer.html",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Infobench",
		"Type": "instruction-following",
		"Description": "Evaluating Large Language Models' (LLMs) ability to follow instructions by breaking complex instructions into simpler criteria, facilitating a detailed analysis of LLMs' compliance with various aspects of tasks.",
		"Benchmark paper": "INFOBENCH: Evaluating Instruction Following Ability in Large Language Models \nhttps://arxiv.org/abs/2401.03601",
		"Code repository": "https://github.com/qinyiwei/InfoBench",
		"Dataset ": "https://huggingface.co/datasets/kqsong/InFoBench",
		"Number of examples": "500",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "LLM-AggreFact",
		"Type": "safety",
		"Description": "Fact verification benchmark that aggregates 11 publicly available datasets on factual consistency evaluation across both closed-book and grounded generation settings.",
		"Benchmark paper": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents \nhttps://arxiv.org/abs/2404.10774 ",
		"Code repository": "https://github.com/Liyan06/MiniCheck",
		"Dataset ": "https://huggingface.co/datasets/lytang/LLM-AggreFact",
		"Number of examples": "59740",
		"License": "CC-BY-ND-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "ACPBench",
		"Type": "agents & tools use,language & reasoning",
		"Description": "A benchmark for evaluating the reasoning tasks in the field of planning. The benchmark consists of 7 reasoning tasks over 13 planning domains.",
		"Benchmark paper": "ACPBench: Reasoning about Action, Change, and Planning\nhttps://arxiv.org/abs/2410.05669",
		"Code repository": "https://github.com/ibm/ACPBench",
		"Dataset ": "https://huggingface.co/datasets/ibm-research/acp_bench",
		"Number of examples": "3210",
		"License": "CDLA-Permissive-2.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "DetectRL",
		"Type": "llm-generated text detection",
		"Description": "Human-written datasets from domains where LLMs are particularly prone to misuse.",
		"Benchmark paper": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios \nhttps://arxiv.org/abs/2410.23746",
		"Code repository": "https://github.com/NLP2CT/DetectRL",
		"Dataset ": "see repo",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Reveal ",
		"Type": "language & reasoning",
		"Description": "A dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings",
		"Benchmark paper": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains \nhttps://arxiv.org/abs/2402.00559",
		"Code repository": "https://reveal-dataset.github.io",
		"Dataset ": "https://huggingface.co/datasets/google/reveal",
		"Number of examples": "6102",
		"License": "CC-BY-ND-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "FlowBench",
		"Type": "agents & tools use",
		"Description": "A benchmark for workflow-guided planning that covers 51 different scenarios from 6 domains, with knowledge presented in text, code, and flowchart formats.",
		"Benchmark paper": "FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents \nhttps://arxiv.org/abs/2406.14884",
		"Code repository": "https://github.com/Justherozen/FlowBench",
		"Dataset ": "see repo",
		"Number of examples": "5313",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "ConflictBank",
		"Type": "knowledge",
		"Description": "Evaluates knowledge conflicts from three aspects: 1) conflicts in retrieved knowledge, 2) conflicts within the models\u2019 encoded knowledge, and 3) the interplay between these conflict forms. ",
		"Benchmark paper": "ConflictBank: A Benchmark for Evaluating Knowledge Conflicts in Large Language Models \nhttps://arxiv.org/html/2408.12076v1",
		"Code repository": "https://github.com/zhaochen0110/conflictbank",
		"Dataset ": "see repo",
		"Number of examples": "553000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "AutoTools",
		"Type": "agents & tools use",
		"Description": "A framework that enables LLMs to automate the tool-use workflow.",
		"Benchmark paper": "Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents \nhttps://arxiv.org/abs/2405.16533",
		"Code repository": "https://github.com/mangopy/Tool-learning-in-the-wild",
		"Dataset ": "see repo",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "LongBench",
		"Type": "language & reasoning",
		"Description": "Assesses the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. Consists of multiple-choice questions, with contexts ranging from 8k to 2M words.",
		"Benchmark paper": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks \nhttps://arxiv.org/abs/2412.15204",
		"Code repository": "https://github.com/THUDM/LongBench",
		"Dataset ": "https://huggingface.co/datasets/THUDM/LongBench-v2",
		"Number of examples": "503",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "InfiniteBench",
		"Type": "language & reasoning",
		"Description": "Evaluates the capabilities of language models to process, understand, and reason over super long contexts (100k+ tokens).",
		"Benchmark paper": "\u221eBench: Extending Long Context Evaluation Beyond 100K Tokens \nhttps://arxiv.org/abs/2402.13718",
		"Code repository": "https://github.com/OpenBMB/InfiniteBench",
		"Dataset ": "https://huggingface.co/datasets/xinrongzhang2022/InfiniteBench",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "LOFT",
		"Type": "language & reasoning,information retrieval & RAG,multimodal",
		"Description": "A benchmark of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs' performance on in-context retrieval and reasoning. ",
		"Benchmark paper": "Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More? \nhttps://arxiv.org/abs/2406.13121",
		"Code repository": "https://github.com/google-deepmind/loft",
		"Dataset ": "https://github.com/google-deepmind/loft?tab=readme-ov-file#datasets",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "WorfBench",
		"Type": "agents & tools use",
		"Description": "Unified workflow generation benchmark with multi-faceted scenarios and graph workflow structures.",
		"Benchmark paper": "Benchmarking Agentic Workflow Generation \nhttps://arxiv.org/abs/2410.07869 ",
		"Code repository": "https://github.com/zjunlp/WorfBench",
		"Dataset ": "https://huggingface.co/collections/zjunlp/worfbench-66fc28b8ac1c8e2672192ea1",
		"Number of examples": "21000",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Arena-Hard ",
		"Type": "conversation & chatbots",
		"Description": "Automatic evaluation tool for instruction-tuned LLMs, contains 500 challenging user queries sourced from Chatbot Arena. ",
		"Benchmark paper": "From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline\nhttps://arxiv.org/abs/2406.11939",
		"Code repository": "https://github.com/lmarena/arena-hard-auto",
		"Dataset ": "https://huggingface.co/spaces/lmarena-ai/arena-hard-browser",
		"Number of examples": "500",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "TemplateGSM",
		"Type": "math",
		"Description": "A dataset comprising over 7 million synthetically generated grade school math problems, each accompanied by code-based and natural language solutions.",
		"Benchmark paper": "Training and Evaluating Language Models with Template-based Data Generation \nhttps://arxiv.org/abs/2411.18104",
		"Code repository": "https://github.com/iiis-ai/TemplateMath",
		"Dataset ": "https://huggingface.co/datasets/math-ai/TemplateGSM",
		"Number of examples": "7000000",
		"License": "CC-BY-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "HARD-Math",
		"Type": "math",
		"Description": "Human-Annotated Reasoning Dataset for Math. Consists of short answer problems, based on the AHSME, AMC, & AIME contests.",
		"Benchmark paper": "HARDMath: A Benchmark Dataset for Challenging Problems in Applied Mathematics \nhttps://arxiv.org/abs/2410.09988",
		"Code repository": "https://github.com/sarahmart/HARDMath",
		"Dataset ": "https://github.com/sarahmart/HARDMath/tree/main/data",
		"Number of examples": "1400",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Loong",
		"Type": "information retrieval & RAG",
		"Description": "Long-context benchmark, aligning with realistic scenarios through extended multi-document question answering (QA).",
		"Benchmark paper": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA \nhttps://arxiv.org/abs/2406.17419",
		"Code repository": "https://github.com/MozerWang/Loong",
		"Dataset ": "(in Chinese) https://modelscope.cn/datasets/iic/Loong",
		"Number of examples": "1600",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "SWE-bench verified",
		"Type": "coding",
		"Description": "A subset of SWE-bench, consisting of 500 samples verified to be non-problematic by our human annotators.",
		"Benchmark paper": "Introducing SWE-bench Verified \nhttps://openai.com/index/introducing-swe-bench-verified/",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified",
		"Number of examples": "500",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "API-Bank",
		"Type": "agents & tools use",
		"Description": "Specifically designed for tool-augmented LLMs.",
		"Benchmark paper": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs\nhttps://arxiv.org/abs/2304.08244",
		"Code repository": "https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank",
		"Dataset ": "https://huggingface.co/datasets/liminghao1630/API-Bank",
		"Number of examples": "n/a",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "CrossCodeEval",
		"Type": "coding",
		"Description": "Multilingual code completion tasks built on built on real-world GitHub repositories in Python, Java, TypeScript, and C#.",
		"Benchmark paper": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion\nhttps://arxiv.org/abs/2310.11248",
		"Code repository": "https://github.com/amazon-science/cceval",
		"Dataset ": "https://github.com/amazon-science/cceval/tree/main/data",
		"Number of examples": "10000",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "SEED-Bench",
		"Type": "multimodal",
		"Description": "A benchmark for evaluating Multimodal LLMs using multiple-choice questions.",
		"Benchmark paper": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension\nhttps://arxiv.org/abs/2307.16125",
		"Code repository": "https://github.com/AILab-CVC/SEED-Bench",
		"Dataset ": "https://huggingface.co/datasets/AILab-CVC/SEED-Bench-2",
		"Number of examples": "24000",
		"License": "CC-BY-NC-4.0",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "Chain-of-Thought Hub",
		"Type": "language & reasoning",
		"Description": "Curated complex reasoning tasks including math, science, coding, long-context.",
		"Benchmark paper": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance\nhttps://arxiv.org/abs/2305.17306",
		"Code repository": "https://github.com/FranxYao/chain-of-thought-hub/",
		"Dataset ": "see repository ",
		"Number of examples": "1000+",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "ForbiddenQuestions",
		"Type": "safety",
		"Description": "A set of questions targetting 13 behavior scenarios disallowed by OpenAI.",
		"Benchmark paper": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models\nhttps://arxiv.org/abs/2308.03825",
		"Code repository": "https://github.com/verazuo/jailbreak_llms",
		"Dataset ": "https://github.com/verazuo/jailbreak_llms",
		"Number of examples": "15140",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "MuSR",
		"Type": "language & reasoning",
		"Description": "Multistep reasoning tasks based on text narratives (e.g., 1000 words murder mysteries).",
		"Benchmark paper": "https://arxiv.org/abs/2310.16049",
		"Code repository": "https://github.com/Zayne-sprague/MuSR",
		"Dataset ": "https://github.com/Zayne-sprague/MuSR/tree/main/datasets",
		"Number of examples": "756",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "ToolLLM",
		"Type": "agents & tools use",
		"Description": "An instruction-tuning dataset for tool use.",
		"Benchmark paper": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs\nhttps://arxiv.org/abs/2307.16789",
		"Code repository": "https://github.com/OpenBMB/ToolBench",
		"Dataset ": "https://github.com/OpenBMB/ToolBench?tab=readme-ov-file#data-release",
		"Number of examples": "n/a",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "FreshQA",
		"Type": "knowledge",
		"Description": "Tests factuality of LLM-generated text in the context of answering questions that test current world knowledge. The dataset is updated weekly.",
		"Benchmark paper": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation\nhttps://arxiv.org/abs/2310.03214",
		"Code repository": "https://github.com/freshllms/freshqa",
		"Dataset ": "https://github.com/freshllms/freshqa?tab=readme-ov-file#freshqa",
		"Number of examples": "599",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "MT-Bench",
		"Type": "conversation & chatbots",
		"Description": "Multi-turn questions: an open-ended question and a follow-up question.",
		"Benchmark paper": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena \nhttps://arxiv.org/abs/2306.05685",
		"Code repository": "https://github.com/lm-sys/FastChat/tree/main",
		"Dataset ": "https://huggingface.co/datasets/lmsys/mt_bench_human_judgments",
		"Number of examples": "3300",
		"License": "CC-BY-4.0",
		"Initial publication year": "2023",
		"Cited by:": ">500"
	},
	{
		"Name": "ToolBench",
		"Type": "agents & tools use",
		"Description": "A tool manipulation benchmark consisting of software tools for real-world tasks.",
		"Benchmark paper": "On the Tool Manipulation Capability of Open-source Large Language Models\nhttps://arxiv.org/abs/2305.16504",
		"Code repository": "https://github.com/sambanova/toolbench/tree/main",
		"Dataset ": "https://github.com/sambanova/toolbench/tree/main",
		"Number of examples": "n/a",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "AgentBench",
		"Type": "agents & tools use",
		"Description": "Evaluate LLM-as-Agent across 8 environments, including Operating System (OS)\nDatabase (DB), Knowledge Graph (KG), Digital Card Game (DCG), and Lateral Thinking Puzzles (LTP).",
		"Benchmark paper": "AgentBench: Evaluating LLMs as Agents\nhttps://arxiv.org/abs/2308.03688",
		"Code repository": "https://github.com/THUDM/AgentBench",
		"Dataset ": "https://github.com/THUDM/AgentBench/tree/main/data",
		"Number of examples": "1360",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "Q-bench",
		"Type": "multimodal",
		"Description": "Evaluates MLLMs on three dimensions: low-level visual perception, low-level visual description, and overall visual quality assessment.",
		"Benchmark paper": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision\nhttps://arxiv.org/abs/2309.14181",
		"Code repository": "https://github.com/Q-Future/Q-Bench",
		"Dataset ": "https://huggingface.co/datasets/q-future/Q-Bench-HF",
		"Number of examples": "2990",
		"License": "S-Lab License 1.0",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "EvalPlus",
		"Type": "coding",
		"Description": "Extended HumanEval & MBPP by 80x/35x for rigorous eval.",
		"Benchmark paper": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation\nhttps://arxiv.org/abs/2305.01210",
		"Code repository": "https://github.com/evalplus/evalplus",
		"Dataset ": "https://github.com/evalplus/evalplus/tree/master/evalplus/data",
		"Number of examples": "n/a",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": ">500"
	},
	{
		"Name": "MaliciousInstruct",
		"Type": "safety",
		"Description": "Covers ten 'malicious intentions', including psychological manipulation, theft, cyberbullying, and fraud.",
		"Benchmark paper": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation\nhttps://arxiv.org/abs/2310.06987",
		"Code repository": "https://github.com/Princeton-SysML/Jailbreak_LLM/tree/main",
		"Dataset ": "https://github.com/Princeton-SysML/Jailbreak_LLM/tree/main/data",
		"Number of examples": "100",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "SycophancyEval",
		"Type": "safety",
		"Description": "Tests if human feedback encourages model responses to match user beliefs over truthful ones, a behavior known as sycophancy.",
		"Benchmark paper": "Towards Understanding Sycophancy in Language Models\nhttps://arxiv.org/abs/2310.13548",
		"Code repository": "https://github.com/meg-tong/sycophancy-eval",
		"Dataset ": "https://huggingface.co/datasets/meg-tong/sycophancy-eval",
		"Number of examples": "n/a",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "DecodingTrust",
		"Type": "safety",
		"Description": "Evaluate trustworthiness of LLMs across 8 perspectives: toxicity, stereotypes, adversarial and robustness, privacy, ethics and fairness.",
		"Benchmark paper": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models\nhttps://arxiv.org/abs/2306.11698",
		"Code repository": "https://github.com/AI-secure/DecodingTrust",
		"Dataset ": "https://huggingface.co/datasets/AI-Secure/DecodingTrust",
		"Number of examples": "243,877",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "AdvBench",
		"Type": "safety",
		"Description": "A set of 500 harmful strings that the model should not reproduce and 500 harmful instructions.",
		"Benchmark paper": "Universal and Transferable Adversarial Attacks on Aligned Language Models\nhttps://arxiv.org/abs/2307.15043",
		"Code repository": "https://github.com/llm-attacks/llm-attacks",
		"Dataset ": "https://github.com/llm-attacks/llm-attacks/tree/main/data/advbench",
		"Number of examples": "1000",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": ">500"
	},
	{
		"Name": "XSTest",
		"Type": "safety",
		"Description": "A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models.",
		"Benchmark paper": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models\nhttps://arxiv.org/abs/2308.01263",
		"Code repository": "https://github.com/paul-rottger/exaggerated-safety",
		"Dataset ": "https://github.com/paul-rottger/exaggerated-safety/blob/main/xstest_v2_prompts.csv",
		"Number of examples": "450",
		"License": "CC-BY-4.0",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "ClassEval",
		"Type": "coding",
		"Description": "Class-level Python code generation tasks.",
		"Benchmark paper": "ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation\"\nhttps://arxiv.org/abs/2308.01861",
		"Code repository": "https://github.com/FudanSELab/ClassEval",
		"Dataset ": "https://huggingface.co/datasets/FudanSELab/ClassEval",
		"Number of examples": "100",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "MetaTool",
		"Type": "agents & tools use",
		"Description": "A set of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios.",
		"Benchmark paper": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use\nhttps://arxiv.org/abs/2310.03128",
		"Code repository": "https://github.com/HowieHwong/MetaTool",
		"Dataset ": "https://github.com/HowieHwong/MetaTool/tree/master/dataset",
		"Number of examples": "20879",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "M3Exam",
		"Type": "multimodal",
		"Description": "A set of human exam questions in 9 diverse languages with three educational levels, where about 23% of the questions require processing images for successful solving.",
		"Benchmark paper": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models\nhttps://arxiv.org/abs/2306.05179",
		"Code repository": "https://github.com/DAMO-NLP-SG/M3Exam",
		"Dataset ": "https://github.com/DAMO-NLP-SG/M3Exam?tab=readme-ov-file#data",
		"Number of examples": "12317",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "OpinionQA",
		"Type": "safety",
		"Description": "A dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups.",
		"Benchmark paper": "Whose Opinions Do Language Models Reflect?\nhttps://arxiv.org/abs/2303.17548",
		"Code repository": "https://github.com/tatsu-lab/opinions_qa",
		"Dataset ": "https://worksheets.codalab.org/worksheets/0x6fb693719477478aac73fc07db333f69",
		"Number of examples": "1498",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "SafetyBench",
		"Type": "safety",
		"Description": "Multiple-choice questions concerning offensive content, bias, illegal activities, and mental health.",
		"Benchmark paper": "SafetyBench: Evaluating the Safety of Large Language Models\nhttps://arxiv.org/abs/2309.07045",
		"Code repository": "https://github.com/thu-coai/SafetyBench",
		"Dataset ": "https://huggingface.co/datasets/thu-coai/SafetyBench",
		"Number of examples": "11435",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "GPQA",
		"Type": "language & reasoning",
		"Description": "A set of multiple-choice questions written by domain experts in biology, physics, and chemistry.",
		"Benchmark paper": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark\nhttps://arxiv.org/abs/2311.12022",
		"Code repository": "https://github.com/idavidrein/gpqa",
		"Dataset ": "https://huggingface.co/datasets/Idavidrein/gpqa",
		"Number of examples": "448",
		"License": "CC-BY-4.0",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "Repobench",
		"Type": "coding",
		"Description": "Consists of three interconnected evaluation tasks: retrieve the most relevant code snippets, predict the next line of code, and handle complex tasks that require a combination of both retrieval and next-line prediction. Supports both Python and Java.",
		"Benchmark paper": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems\nhttps://arxiv.org/abs/2306.03091",
		"Code repository": "https://github.com/Leolty/repobench",
		"Dataset ": "https://huggingface.co/datasets/tianyang/repobench-r \nhttps://huggingface.co/datasets/tianyang/repobench-c \nhttps://huggingface.co/datasets/tianyang/repobench-p",
		"Number of examples": "unspecified",
		"License": "CC-BY-NC-ND 4.0",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "IFEval",
		"Type": "language & reasoning,instruction-following",
		"Description": "A set of prompts with verifiable instructions, such as \"write in more than 400 words\".",
		"Benchmark paper": "https://arxiv.org/abs/2311.07911",
		"Code repository": "https://github.com/google-research/google-research/tree/master/instruction_following_eval",
		"Dataset ": "https://github.com/google-research/google-research/tree/master/instruction_following_eval",
		"Number of examples": "500",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "AGIEval",
		"Type": "language & reasoning",
		"Description": "A collection of standardized tests, including GRE, GMAT, SAT, LSAT.",
		"Benchmark paper": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models\nhttps://arxiv.org/abs/2304.06364",
		"Code repository": "https://github.com/ruixiangcui/AGIEval/tree/main",
		"Dataset ": "https://github.com/ruixiangcui/AGIEval/tree/main/data\n",
		"Number of examples": "n/a",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "HarmfulQA",
		"Type": "safety",
		"Description": "Harmful questions covering 10 topics and ~10 subtopics each.",
		"Benchmark paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment\nhttps://arxiv.org/abs/2308.09662",
		"Code repository": "https://github.com/declare-lab/red-instruct",
		"Dataset ": "https://huggingface.co/datasets/declare-lab/HarmfulQA",
		"Number of examples": "1960",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "QHarm",
		"Type": "safety",
		"Description": "Dataset consists of human-written entries sampled randomly from AnthropicHarmlessBase.",
		"Benchmark paper": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions\nhttps://arxiv.org/abs/2309.07875",
		"Code repository": "https://github.com/vinid/safety-tuned-llamas",
		"Dataset ": "https://github.com/vinid/safety-tuned-llamas",
		"Number of examples": "100",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "LegalBench",
		"Type": "domain-specific",
		"Description": "Collaboratively curated tasks for evaluating legal reasoning in English LLMs.",
		"Benchmark paper": "LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models\nhttps://arxiv.org/abs/2308.11462",
		"Code repository": "https://github.com/HazyResearch/legalbench/",
		"Dataset ": "https://huggingface.co/datasets/nguha/legalbench",
		"Number of examples": "162",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "MMMU",
		"Type": "multimodal,language & reasoning",
		"Description": "Evaluates multimodal models on massive multi-discipline tasks demanding college-level subject knowledge. Includes 11.5K questions from college exams, quizzes, and textbooks (https://mmmu-benchmark.github.io/).",
		"Benchmark paper": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI\nhttps://arxiv.org/abs/2311.16502",
		"Code repository": "https://github.com/MMMU-Benchmark/MMMU",
		"Dataset ": "https://huggingface.co/datasets/MMMU/MMMU",
		"Number of examples": "11500",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT",
		"Type": "multimodal",
		"Description": "A framework for quantitatively evaluating interactive LLMs such as ChatGPT using 23 data sets covering 8 common NLP tasks.",
		"Benchmark paper": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity\nhttps://arxiv.org/abs/2302.04023",
		"Code repository": "https://github.com/HLTCHKUST/chatgpt-evaluation",
		"Dataset ": "https://github.com/HLTCHKUST/chatgpt-evaluation/tree/main/src",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": ">500"
	},
	{
		"Name": "SWE-bench",
		"Type": "coding",
		"Description": "Real-world software issues collected from GitHub.",
		"Benchmark paper": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?\nhttps://arxiv.org/abs/2310.06770",
		"Code repository": "https://github.com/princeton-nlp/SWE-bench",
		"Dataset ": "https://huggingface.co/datasets/princeton-nlp/SWE-bench",
		"Number of examples": "2200",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "Webarena",
		"Type": "agents & tools use",
		"Description": "An environment for autonomous agents that perform tasks on the web.",
		"Benchmark paper": "WebArena: A Realistic Web Environment for Building Autonomous Agents\nhttps://arxiv.org/abs/2307.13854",
		"Code repository": "https://github.com/web-arena-x/webarena",
		"Dataset ": "https://github.com/web-arena-x/webarena/blob/main/config_files/test.raw.json",
		"Number of examples": "n/a",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "BeaverTails",
		"Type": "safety",
		"Description": "A set of prompts sampled from AnthropicRedTeam that cover 14 harm categories.",
		"Benchmark paper": "BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset\nhttps://arxiv.org/abs/2307.04657",
		"Code repository": "https://github.com/PKU-Alignment/beavertails",
		"Dataset ": "https://huggingface.co/datasets/PKU-Alignment/BeaverTails",
		"Number of examples": "334000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "Code Lingua",
		"Type": "coding",
		"Description": "Compares the ability of LLMs to understand what the code implements in source language and translate the same semantics in target language.",
		"Benchmark paper": "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code\nhttps://arxiv.org/abs/2308.03109",
		"Code repository": "https://github.com/codetlingua/codetlingua",
		"Dataset ": "https://huggingface.co/iidai",
		"Number of examples": "1700",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "SummEdits",
		"Type": "language & reasoning",
		"Description": "Inconsistency detection in summaries",
		"Benchmark paper": "https://arxiv.org/abs/2305.14540",
		"Code repository": "https://github.com/salesforce/factualNLG",
		"Dataset ": "https://github.com/salesforce/factualNLG/tree/master/data/summedits",
		"Number of examples": "6,348",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "EvalCrafter",
		"Type": "video,multimodal",
		"Description": "A framework and pipeline for evaluating the performance of the generated videos, such as visual qualities, content qualities, motion qualities, and text-video alignment.",
		"Benchmark paper": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models\nhttps://arxiv.org/abs/2310.11440",
		"Code repository": "https://github.com/EvalCrafter/EvalCrafter",
		"Dataset ": "https://huggingface.co/datasets/RaphaelLiu/EvalCrafter_T2V_Dataset",
		"Number of examples": "700",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "MME",
		"Type": "multimodal",
		"Description": "Measures both perception and cognition abilities on a total of 14 subtasks.",
		"Benchmark paper": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\nhttps://arxiv.org/abs/2306.13394",
		"Code repository": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation",
		"Dataset ": "https://huggingface.co/datasets/lmms-lab/MME",
		"Number of examples": "2374",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": ">500"
	},
	{
		"Name": "DoNotAnswer",
		"Type": "safety",
		"Description": "The dataset consists of prompts across 12 harm types to which responsible LLMs do not answer. ",
		"Benchmark paper": "Do-Not-Answer: Evaluating Safeguards in LLMs\nhttps://arxiv.org/abs/2308.13387",
		"Code repository": "https://github.com/Libr-AI/do-not-answer",
		"Dataset ": "https://huggingface.co/datasets/LibrAI/do-not-answer",
		"Number of examples": "939",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "ToolQA",
		"Type": "agents & tools use",
		"Description": "A new dataset to evaluate the capabilities of LLMs in answering challenging questions with external tools. It offers two levels (easy/hard) across eight real-life scenarios.",
		"Benchmark paper": "ToolQA: A Dataset for LLM Question Answering with External Tools \nhttps://arxiv.org/abs/2306.13304",
		"Code repository": "https://github.com/night-chen/ToolQA ",
		"Dataset ": "see repo",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "TRUSTGPT",
		"Type": "safety,bias & ethics",
		"Description": "Evaluates large language models on toxicity, bias, and value-alignment to ensure ethical and moral compliance.",
		"Benchmark paper": "TRUSTGPT: A Benchmark for Trustworthy and\nResponsible Large Language Models\nhttps://arxiv.org/pdf/2306.11507",
		"Code repository": "https://github.com/HowieHwong/TrustGPT",
		"Dataset ": "https://github.com/mbforbes/social-chemistry-101",
		"Number of examples": "292000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "T-Eval",
		"Type": "agents & tools use",
		"Description": "Decomposes the tool utilization capability into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review.",
		"Benchmark paper": "T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step \nhttps://arxiv.org/abs/2312.14033 ",
		"Code repository": "https://github.com/open-compass/T-Eval",
		"Dataset ": "https://huggingface.co/datasets/lovesnowbest/T-Eval",
		"Number of examples": "23305",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "GAIA",
		"Type": "agents & tools use",
		"Description": "Presents real-world questions requiring reasoning, multi-modality handling, and tool-use proficiency to evaluate general AI assistants.",
		"Benchmark paper": "GAIA: A Benchmark for General AI Assistants \nhttps://arxiv.org/pdf/2311.12983 ",
		"Code repository": "https://huggingface.co/gaia-benchmark",
		"Dataset ": "https://huggingface.co/datasets/gaia-benchmark/GAIA",
		"Number of examples": "450",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "LVLM-eHub",
		"Type": "multimodal",
		"Description": "Multi-Modality Arena helps benchmark vision-language models side-by-side while providing images as inputs. ",
		"Benchmark paper": "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models \nhttps://arxiv.org/abs/2306.09265",
		"Code repository": "https://github.com/OpenGVLab/Multi-Modality-Arena",
		"Dataset ": "see repo",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "ARB",
		"Type": "language & reasoning,knowledge",
		"Description": "Advanced reasoning problems in math, physics, biology, chemistry, and law.",
		"Benchmark paper": "ARB: Advanced Reasoning Benchmark for Large Language Models\nhttps://arxiv.org/abs/2307.13692",
		"Code repository": "https://github.com/TheDuckAI/arb?tab=readme-ov-file",
		"Dataset ": "https://advanced-reasoning-benchmark.netlify.app/documentation",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "RAGTruth",
		"Type": "information retrieval & RAG,safety",
		"Description": "A corpus tailored for analyzing word-level hallucinations within the standard RAG frameworks for LLM applications. RAGTruth comprises 18,000 naturally generated responses from diverse LLMs using RAG.",
		"Benchmark paper": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models \nhttps://arxiv.org/abs/2401.00396",
		"Code repository": "https://github.com/ParticleMedia/RAGTruth",
		"Dataset ": "https://huggingface.co/datasets/wandb/RAGTruth-processed",
		"Number of examples": "18000",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "AGIEval",
		"Type": "language & reasoning,knowledge",
		"Description": "Uses human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests.",
		"Benchmark paper": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models https://arxiv.org/pdf/2304.06364",
		"Code repository": "https://github.com/ruixiangcui/AGIEval",
		"Dataset ": "see repo",
		"Number of examples": "8000",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "SOCKET",
		"Type": "bias & ethics,knowledge",
		"Description": "A theory-driven benchmark containing 58 NLP tasks testing social knowledge, including humor, sarcasm, offensiveness, sentiment, emotion, and trustworthiness.",
		"Benchmark paper": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with the SOCKET Benchmark. https://arxiv.org/pdf/2305.14938  ",
		"Code repository": "https://github.com/minjechoi/SOCKET",
		"Dataset ": "https://huggingface.co/datasets/Blablablab/SOCKET/tree/main/SOCKET_DATA ",
		"Number of examples": "58",
		"License": "CC-BY-4.0",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "ExpertQA",
		"Type": "safety",
		"Description": "Long-form QA dataset with 2177 questions spanning 32 fields for evaluating attribution and factuality of LLM outputs in domain-specific scenarios.",
		"Benchmark paper": "ExpertQA: Expert-Curated Questions and Attributed Answers \nhttps://arxiv.org/abs/2309.07852",
		"Code repository": "https://github.com/chaitanyamalaviya/ExpertQA",
		"Dataset ": "see repo",
		"Number of examples": "2177",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "MINT",
		"Type": "agents & tools use",
		"Description": "Evaluates LLMs' ability to solve tasks with multi-turn interactions by using tools and leveraging natural language feedback.",
		"Benchmark paper": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback\nhttps://arxiv.org/abs/2309.10691 ",
		"Code repository": "https://github.com/xingyaoww/mint-bench",
		"Dataset ": "https://github.com/xingyaoww/mint-bench/blob/main/docs/DATA.md",
		"Number of examples": "586",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "HaluEval",
		"Type": "safety",
		"Description": "A collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination.",
		"Benchmark paper": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models \nhttps://arxiv.org/abs/2305.11747",
		"Code repository": "https://github.com/RUCAIBox/HaluEval",
		"Dataset ": "https://github.com/RUCAIBox/HaluEval?tab=readme-ov-file#data-release",
		"Number of examples": "35000",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "ToolEmu",
		"Type": "safety,agents & tools use",
		"Description": "A framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation.",
		"Benchmark paper": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox \nhttps://arxiv.org/abs/2309.15817",
		"Code repository": "https://github.com/ryoungj/ToolEmu",
		"Dataset ": "https://github.com/ryoungj/ToolEmu/blob/main/assets/all_cases.json",
		"Number of examples": "144",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "MMBench",
		"Type": "multimodal",
		"Description": "A bilingual benchmark for assessing the multi-modal capabilities of vision-language models. Contains 2974 multiple-choice questions, covering 20 ability dimensions.",
		"Benchmark paper": "MMBench: Is Your Multi-modal Model an All-around Player? \nhttps://arxiv.org/abs/2307.06281",
		"Code repository": "https://github.com/open-compass/MMBench",
		"Dataset ": "see repo",
		"Number of examples": "2974",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": ">500"
	},
	{
		"Name": "EmotionBench",
		"Type": "empathy",
		"Description": "Evaluates the empathy ability of LLMs across 8 emotions: anger, anxiety, depression, frustration, jealousy, guilt, fear, embarrassment.",
		"Benchmark paper": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench \nhttps://arxiv.org/abs/2308.03656",
		"Code repository": "https://github.com/CUHK-ARISE/EmotionBench",
		"Dataset ": "https://huggingface.co/datasets/CUHK-ARISE/EmotionBench",
		"Number of examples": "400",
		"License": "Apache-2.0 license",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "EQ-Bench",
		"Type": "empathy",
		"Description": "Assesses the ability of LLMs to understand complex emotions and social interactions by asking them to predict the intensity of emotional states of characters in a dialogue.",
		"Benchmark paper": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models \nhttps://arxiv.org/abs/2312.06281",
		"Code repository": "https://github.com/EQ-bench/EQ-Bench",
		"Dataset ": "https://huggingface.co/datasets/pbevan11/EQ-Bench",
		"Number of examples": "171",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "RED-EVAL",
		"Type": "safety",
		"Description": "Safety evaluation benchmark that carries out red-teaming.",
		"Benchmark paper": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment\nhttps://arxiv.org/abs/2308.09662",
		"Code repository": "https://github.com/declare-lab/red-instruct",
		"Dataset ": "see repo",
		"Number of examples": "1960",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "WiCE",
		"Type": "information retrieval & RAG",
		"Description": "Textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia.",
		"Benchmark paper": "WiCE: Real-World Entailment for Claims in Wikipedia \nhttps://arxiv.org/abs/2303.01432",
		"Code repository": "https://github.com/ryokamoi/wice",
		"Dataset ": "https://huggingface.co/datasets/tasksource/wice",
		"Number of examples": "5377",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2023",
		"Cited by:": "51-100"
	},
	{
		"Name": "TheoremQA\t",
		"Type": "math",
		"Description": "Theorem-driven QA dataset that evaluates LLMs capabilities to apply theorems to solve science problems. Contains 800 questions covering 350 theorems from math, physics, EE&CS, and finance.",
		"Benchmark paper": "TheoremQA: A Theorem-driven Question Answering dataset \nhttps://arxiv.org/abs/2305.12524",
		"Code repository": "https://github.com/TIGER-AI-Lab/TheoremQA",
		"Dataset ": "https://huggingface.co/datasets/TIGER-Lab/TheoremQA",
		"Number of examples": "800",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "LFQA-Verification",
		"Type": "information retrieval & RAG",
		"Description": "Tests how retrieval augmentation impacts different LMs. Compares answers generated while using the same evidence documents by different LMs, and how differing quality of retrieval documents impacts the answers generated from the same LM.",
		"Benchmark paper": "Understanding Retrieval Augmentation for Long-Form Question Answering \nhttps://arxiv.org/abs/2310.12150",
		"Code repository": "https://github.com/timchen0618/LFQA-Verification/",
		"Dataset ": "https://github.com/timchen0618/LFQA-Verification/tree/main/data",
		"Number of examples": "100",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "NPHardEval",
		"Type": "language & reasoning",
		"Description": "Evaluates the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class.",
		"Benchmark paper": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes \nhttps://arxiv.org/abs/2312.14890",
		"Code repository": "https://github.com/casmlab/NPHardEval",
		"Dataset ": "https://github.com/casmlab/NPHardEval",
		"Number of examples": "900",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "AgentBench ",
		"Type": "agents & tools use",
		"Description": "A multi-dimensional benchmark to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting.",
		"Benchmark paper": "AgentBench: Evaluating LLMs as Agents \nhttps://arxiv.org/abs/2308.03688",
		"Code repository": "https://github.com/THUDM/AgentBench",
		"Dataset ": "https://github.com/THUDM/AgentBench/tree/main/data",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "PandaLM",
		"Type": "instruction-following",
		"Description": "A judge large language model which is trained to distinguish the superior model given several LLMs. It compares the responses of different LLMs and provide a reason for the decision, along with a reference answer.",
		"Benchmark paper": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\nhttps://arxiv.org/abs/2306.05087",
		"Code repository": "https://github.com/WeOpenML/PandaLM",
		"Dataset ": "https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL3UvYy8xZDM3ZWRlNmVhYTk3NGRkL0VkMTBxZXJtN1RjZ2dCMnJBZ0FBQUFBQk5hbTM2YVExNlpjTU1IMjFaVU85ZlE%5FZT1nTjZueFI&cid=1D37EDE6EAA974DD&id=1D37EDE6EAA974DD%21683&parId=1D37EDE6EAA974DD%21682&o=OneUp",
		"Number of examples": "1000",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "DocMath-Eval",
		"Type": "math,language & reasoning",
		"Description": "Designed to evaluate the numerical reasoning capabilities of LLMs in the context of understanding and analyzing specialized documents containing both text and tables. ",
		"Benchmark paper": "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in Understanding Long and Specialized Documents \nhttps://arxiv.org/abs/2311.09805",
		"Code repository": "https://github.com/yale-nlp/DocMath-Eval",
		"Dataset ": "https://huggingface.co/datasets/yale-nlp/DocMath-Eval",
		"Number of examples": "4000",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "ScienceQA",
		"Type": "knowledge,language & reasoning,multimodal",
		"Description": "Multimodal multiple choice questions with diverse science topics and annotations of their answers with corresponding lectures and explanations.",
		"Benchmark paper": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\nhttps://arxiv.org/abs/2209.09513",
		"Code repository": "https://github.com/lupantech/ScienceQA",
		"Dataset ": "https://huggingface.co/datasets/derek-thomas/ScienceQA",
		"Number of examples": "21208",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2022",
		"Cited by:": ">500"
	},
	{
		"Name": "DS-1000",
		"Type": "coding",
		"Description": "Code generation benchmark with data science problems spanning seven Python libraries, such as NumPy and Pandas.",
		"Benchmark paper": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation\nhttps://arxiv.org/abs/2211.11501",
		"Code repository": "https://github.com/xlang-ai/DS-1000",
		"Dataset ": "https://huggingface.co/datasets/xlangai/DS-1000",
		"Number of examples": "1000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2022",
		"Cited by:": "101-500"
	},
	{
		"Name": "MedMCQA",
		"Type": "domain-specific",
		"Description": "Four-option multiple-choice questions from Indian medical entrance examinations. Covers 2,400 healthcare topics and 21 medical subjects.",
		"Benchmark paper": "MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering\nhttps://arxiv.org/abs/2203.14371",
		"Code repository": "https://github.com/medmcqa/medmcqa",
		"Dataset ": "https://github.com/medmcqa/medmcqa",
		"Number of examples": "194000",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": "101-500"
	},
	{
		"Name": "ToxiGen",
		"Type": "safety",
		"Description": "A set of toxic and benign statements about minority groups.",
		"Benchmark paper": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection\nhttps://arxiv.org/abs/2203.09509",
		"Code repository": "https://github.com/microsoft/TOXIGEN/tree/main",
		"Dataset ": "https://huggingface.co/datasets/toxigen/toxigen-data",
		"Number of examples": "274000",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": "101-500"
	},
	{
		"Name": "HELM",
		"Type": "language & reasoning,safety",
		"Description": "Reasoning tasks in several domains (reusing other benchmarks) with a focus on multi-metric evaluation (https://crfm.stanford.edu/helm/).",
		"Benchmark paper": "https://arxiv.org/abs/2211.09110",
		"Code repository": "https://github.com/stanford-crfm/helm",
		"Dataset ": "see repository ",
		"Number of examples": "unspecified",
		"License": "Apache-2.0 license",
		"Initial publication year": "2022",
		"Cited by:": ">500"
	},
	{
		"Name": "HHH (Helpfulness, Honesty, Harmlessness)",
		"Type": "safety",
		"Description": "Human preference data about helpfulness and harmlessness.",
		"Benchmark paper": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\nhttps://arxiv.org/abs/2204.05862",
		"Code repository": "https://github.com/anthropics/hh-rlhf",
		"Dataset ": "https://github.com/anthropics/hh-rlhf",
		"Number of examples": "44849",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": ">500"
	},
	{
		"Name": "PersonalInfoLeak",
		"Type": "safety",
		"Description": "Evaluates whether LLMs are prone to leaking PII, contains name-email pairs.",
		"Benchmark paper": "Are Large Pre-Trained Language Models Leaking Your Personal Information?\nhttps://arxiv.org/abs/2205.12628",
		"Code repository": "https://github.com/jeffhj/LM_PersonalInfoLeak",
		"Dataset ": "https://github.com/jeffhj/LM_PersonalInfoLeak/tree/main/data",
		"Number of examples": "3238",
		"License": "Apache-2.0 license",
		"Initial publication year": "2022",
		"Cited by:": "101-500"
	},
	{
		"Name": "e-CARE (explainable CAusal REasoning dataset)",
		"Type": "language & reasoning",
		"Description": "A human-annotated dataset that contains causal reasoning questions.",
		"Benchmark paper": "e-CARE: a New Dataset for Exploring Explainable Causal Reasoning\nhttps://arxiv.org/abs/2205.05849",
		"Code repository": "https://github.com/Waste-Wood/e-CARE",
		"Dataset ": "https://github.com/Waste-Wood/e-CARE/tree/main/dataset",
		"Number of examples": "21000",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": "11-50"
	},
	{
		"Name": "MGSM (Multilingual Grade School Math)",
		"Type": "math",
		"Description": "Grade-school math problems from the GSM8K dataset, translated into 10 languages.",
		"Benchmark paper": "Language Models are Multilingual Chain-of-Thought Reasoners\nhttps://arxiv.org/abs/2210.03057",
		"Code repository": "https://github.com/google-research/url-nlp",
		"Dataset ": "https://huggingface.co/datasets/juletxara/mgsm",
		"Number of examples": "2500",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2022",
		"Cited by:": "101-500"
	},
	{
		"Name": "BigBench Hard",
		"Type": "knowledge,language & reasoning",
		"Description": "A suite of BigBench tasks for which LLMs did not outperform the average human-rater.",
		"Benchmark paper": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them\nhttps://arxiv.org/abs/2210.09261",
		"Code repository": "https://github.com/suzgunmirac/BIG-Bench-Hard",
		"Dataset ": "https://huggingface.co/datasets/maveriq/bigbenchhard",
		"Number of examples": "6500",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": ">500"
	},
	{
		"Name": "PlanBench",
		"Type": "language & reasoning",
		"Description": "A benchmark designed to evaluate the ability of LLMs to generate plans of action and reason about change.",
		"Benchmark paper": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change\nhttps://arxiv.org/abs/2206.10498",
		"Code repository": "https://github.com/karthikv792/LLMs-Planning/tree/main/plan-bench",
		"Dataset ": "https://huggingface.co/datasets/tasksource/planbench",
		"Number of examples": "11113",
		"License": "see dataset page",
		"Initial publication year": "2022",
		"Cited by:": "101-500"
	},
	{
		"Name": "BigBench",
		"Type": "knowledge,language & reasoning",
		"Description": "Set of questions crowdsourced by domain experts in math, biology, physics, and beyond.",
		"Benchmark paper": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models\nhttps://arxiv.org/abs/2206.04615",
		"Code repository": "https://github.com/google/BIG-bench",
		"Dataset ": "https://huggingface.co/datasets/google/bigbench",
		"Number of examples": "n/a",
		"License": "Apache-2.0 license",
		"Initial publication year": "2022",
		"Cited by:": ">500"
	},
	{
		"Name": "AnthropicRedTeam",
		"Type": "safety",
		"Description": "Human-generated and annotated red teaming dialogues.",
		"Benchmark paper": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned\nhttps://arxiv.org/abs/2209.07858",
		"Code repository": "https://github.com/anthropics/hh-rlhf",
		"Dataset ": "https://huggingface.co/datasets/Anthropic/hh-rlhf",
		"Number of examples": "38961",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": "101-500"
	},
	{
		"Name": "GLUE-X",
		"Type": "language & reasoning",
		"Description": "Includes 13 publicly available datasets for Out-of-distribution testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5",
		"Benchmark paper": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective \nhttps://arxiv.org/abs/2211.08073",
		"Code repository": "https://github.com/YangLinyi/GLUE-X",
		"Dataset ": "https://drive.google.com/drive/folders/1BcwjmVOqq96igfbB2MCXwLzthFX7XEhy",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2022",
		"Cited by:": "51-100"
	},
	{
		"Name": "Webshop",
		"Type": "agents & tools use",
		"Description": "A simulated e-commerce website environment with 1.18 million real-world products and 12,087 crowd-sourced text instructions. An agent needs to navigate multiple types of webpages, find, customize, and purchase an item.",
		"Benchmark paper": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents \nhttps://arxiv.org/abs/2207.01206",
		"Code repository": "https://github.com/princeton-nlp/webshop",
		"Dataset ": "https://huggingface.co/datasets/jyang/webshop_inst_goal_pairs_truth",
		"Number of examples": "529107",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": "101-500"
	},
	{
		"Name": "FOLIO",
		"Type": "language & reasoning",
		"Description": "A human-annotated, logically complex dataset for reasoning in natural language, equipped with first-order logic (FOL) annotations.",
		"Benchmark paper": "FOLIO: Natural Language Reasoning with First-Order Logic \nhttps://arxiv.org/abs/2209.00840",
		"Code repository": "https://github.com/Yale-LILY/FOLIO",
		"Dataset ": "https://huggingface.co/datasets/yale-nlp/FOLIO",
		"Number of examples": "1204",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": "101-500"
	},
	{
		"Name": "GSMHard",
		"Type": "math",
		"Description": "The harder version of the GSM8K math reasoning dataset. Numbers in the questions of GSM8K are replaced with larger numbers that are less common.",
		"Benchmark paper": "PAL: Program-aided Language Models \nhttps://arxiv.org/abs/2211.10435",
		"Code repository": "https://github.com/reasoning-machines/pal",
		"Dataset ": "https://huggingface.co/datasets/reasoning-machines/gsm-hard",
		"Number of examples": "1319",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": ">500"
	},
	{
		"Name": "SVAMP",
		"Type": "math",
		"Description": "Grade-school-level math word problems that require models to perform single-variable arithmetic operations. Created by applying variations over examples sampled from existing datasets. ",
		"Benchmark paper": "Are NLP Models really able to Solve Simple Math Word Problems?\nhttps://arxiv.org/abs/2103.07191",
		"Code repository": "https://github.com/arkilpatel/SVAMP",
		"Dataset ": "https://github.com/arkilpatel/SVAMP/tree/main/data",
		"Number of examples": "1000",
		"License": "MIT License",
		"Initial publication year": "2021",
		"Cited by:": ">500"
	},
	{
		"Name": "MATH",
		"Type": "math",
		"Description": "Tasks from US mathematics competitions that cover algebra, calculus, geometry, and statistics.",
		"Benchmark paper": "Measuring Mathematical Problem Solving With the MATH Dataset\nhttps://arxiv.org/abs/2103.03874",
		"Code repository": "https://github.com/hendrycks/math/?tab=readme-ov-file",
		"Dataset ": "https://github.com/hendrycks/math/?tab=readme-ov-file",
		"Number of examples": "12500",
		"License": "MIT License",
		"Initial publication year": "2021",
		"Cited by:": ">500"
	},
	{
		"Name": "BEIR",
		"Type": "language & reasoning,information retrieval & RAG",
		"Description": "BEIR is a heterogeneous benchmark for information retrieval (IR) tasks, contains 15+ IR datasets.",
		"Benchmark paper": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models\nhttps://arxiv.org/abs/2104.08663",
		"Code repository": "https://github.com/beir-cellar/beir",
		"Dataset ": "https://huggingface.co/BeIR",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2021",
		"Cited by:": ">500"
	},
	{
		"Name": "SpartQA",
		"Type": "language & reasoning",
		"Description": "A textual question answering benchmark for spatial reasoning on natural language text.",
		"Benchmark paper": "SpartQA: : A Textual Question Answering Benchmark for Spatial Reasoning\nhttps://arxiv.org/abs/2104.05832",
		"Code repository": "https://github.com/HLR/SpartQA-baselines",
		"Dataset ": "https://github.com/HLR/SpartQA_generation",
		"Number of examples": "510",
		"License": "MIT License",
		"Initial publication year": "2021",
		"Cited by:": "51-100"
	},
	{
		"Name": "TAT-QA\u00a0",
		"Type": "domain-specific",
		"Description": "Questions and associated hybrid contexts from real-world financial reports.",
		"Benchmark paper": "TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance\nhttps://arxiv.org/abs/2105.07624",
		"Code repository": "https://github.com/NExTplusplus/TAT-QA",
		"Dataset ": "https://github.com/NExTplusplus/TAT-QA/tree/master/dataset_raw",
		"Number of examples": "16552",
		"License": "MIT License",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "CodeXGLUE",
		"Type": "coding",
		"Description": "14 datasets for program understanding and generation and three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models.",
		"Benchmark paper": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\nhttps://arxiv.org/abs/2102.04664",
		"Code repository": "https://github.com/microsoft/CodeXGLUE",
		"Dataset ": "https://huggingface.co/datasets?search=code_x_glue",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2021",
		"Cited by:": ">500"
	},
	{
		"Name": "TruthfulQA",
		"Type": "knowledge,language & reasoning,safety",
		"Description": "Evaluates how well models generate truthful responses.",
		"Benchmark paper": "TruthfulQA: Measuring How Models Mimic Human Falsehoods\nhttps://arxiv.org/abs/2109.07958v2",
		"Code repository": "https://github.com/sylinrl/TruthfulQA",
		"Dataset ": "https://huggingface.co/datasets/truthfulqa/truthful_qa",
		"Number of examples": "1634",
		"License": "Apache-2.0 license",
		"Initial publication year": "2021",
		"Cited by:": ">500"
	},
	{
		"Name": "APPS (Automated Programming Progress Standard)",
		"Type": "coding",
		"Description": "A dataset for code generation, including introductory to competitive programming problems.",
		"Benchmark paper": "Measuring Coding Challenge Competence With APPS\nhttps://arxiv.org/abs/2105.09938",
		"Code repository": "https://github.com/hendrycks/apps",
		"Dataset ": "https://huggingface.co/datasets/codeparrot/apps",
		"Number of examples": "10000",
		"License": "MIT License",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "BOLD",
		"Type": "safety,bias & ethics",
		"Description": "A set of unfinished sentences from Wikipedia designed to assess bias in text generation.",
		"Benchmark paper": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation\nhttps://arxiv.org/abs/2101.11718",
		"Code repository": "https://github.com/amazon-science/bold",
		"Dataset ": "https://github.com/amazon-science/bold/tree/main/prompts",
		"Number of examples": "23679",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "BBQ",
		"Type": "safety,bias & ethics",
		"Description": "Evaluate social biases of LLMs in question answering.",
		"Benchmark paper": "BBQ: A Hand-Built Bias Benchmark for Question Answering\nhttps://arxiv.org/abs/2110.08193",
		"Code repository": "https://github.com/nyu-mll/BBQ",
		"Dataset ": "https://github.com/nyu-mll/BBQ/tree/main/data",
		"Number of examples": "58492",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "MBPP (Mostly Basic Programming Problems)",
		"Type": "coding",
		"Description": "Crowd-sourced entry-level programming tasks.",
		"Benchmark paper": "Program Synthesis with Large Language Models\nhttps://arxiv.org/abs/2108.07732",
		"Code repository": "https://github.com/google-research/google-research/blob/master/mbpp/README.md",
		"Dataset ": "https://github.com/google-research/google-research/blob/master/mbpp/mbpp.jsonl",
		"Number of examples": "974",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2021",
		"Cited by:": ">500"
	},
	{
		"Name": "HumanEval",
		"Type": "coding",
		"Description": "Programming tasks and unit tests to check model-generated code.",
		"Benchmark paper": "Evaluating Large Language Models Trained on Code\nhttps://arxiv.org/abs/2107.03374 ",
		"Code repository": "https://github.com/openai/human-eval",
		"Dataset ": "https://huggingface.co/datasets/openai/openai_humaneval",
		"Number of examples": "164",
		"License": "MIT License",
		"Initial publication year": "2021",
		"Cited by:": ">500"
	},
	{
		"Name": "GSM8K",
		"Type": "math",
		"Description": "Grade school math word problems.",
		"Benchmark paper": "Training Verifiers to Solve Math Word Problems\nhttps://arxiv.org/abs/2110.14168",
		"Code repository": "https://github.com/openai/grade-school-math",
		"Dataset ": "https://github.com/openai/grade-school-math/tree/master/grade_school_math/data",
		"Number of examples": "8500",
		"License": "MIT License",
		"Initial publication year": "2021",
		"Cited by:": ">500"
	},
	{
		"Name": "TURINGBENCH",
		"Type": "safety",
		"Description": "The TuringBench Dataset will assist researchers in building models that can effectively distinguish machine-generated texts from human-written texts.",
		"Benchmark paper": "TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation \nhttps://arxiv.org/abs/2109.13296",
		"Code repository": "https://github.com/AdaUchendu/TuringBench",
		"Dataset ": "https://turingbench.ist.psu.edu/",
		"Number of examples": "200000",
		"License": "see dataset page",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "APPS",
		"Type": "coding",
		"Description": "A benchmark dataset for code generation and completion tasks, containing coding problems and solutions.",
		"Benchmark paper": "Measuring Coding Challenge Competence With\nAPPS\nhttps://arxiv.org/pdf/2105.09938",
		"Code repository": "https://github.com/hendrycks/apps",
		"Dataset ": "https://huggingface.co/datasets/codeparrot/apps",
		"Number of examples": "10000",
		"License": "MIT License",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "HHH alignment",
		"Type": "safety",
		"Description": "Evaluates language models on alignment, broken down into the categories of helpfulness, honesty/accuracy, harmlessness, and other.",
		"Benchmark paper": "A General Language Assistant as a Laboratory for Alignment \nhttps://arxiv.org/abs/2112.00861",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/HuggingFaceH4/hhh_alignment",
		"Number of examples": "221",
		"License": "Apache-2.0 license",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "WebQA",
		"Type": "multimodal,language & reasoning",
		"Description": "Visual Question Answering (VQA) benchmark that evaluates models' language groundable visual representations for novel objects and the ability to reason.",
		"Benchmark paper": "WebQA: Multihop and Multimodal QA\nhttps://arxiv.org/abs/2109.00590",
		"Code repository": "https://github.com/WebQnA/WebQA",
		"Dataset ": "https://drive.google.com/drive/folders/1ApfD-RzvJ79b-sLeBx1OaiPNUYauZdAZ",
		"Number of examples": "41732",
		"License": "see dataset page",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "CUAD",
		"Type": "domain-specific",
		"Description": "A dataset for legal contract review with over 13,000 annotations.",
		"Benchmark paper": "CUAD: An Expert-Annotated NLP Dataset for\nLegal Contract Review\nhttps://arxiv.org/pdf/2103.06268",
		"Code repository": "https://github.com/TheAtticusProject/cuad",
		"Dataset ": "https://huggingface.co/datasets/theatticusproject/cuad-qa",
		"Number of examples": "13000",
		"License": "CC-BY-4.0",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "FinQA",
		"Type": "math,language & reasoning",
		"Description": "Large-scale dataset with Question-Answering pairs over Financial reports, written by financial experts.",
		"Benchmark paper": "FinQA: A Dataset of Numerical Reasoning over Financial Data \nhttps://arxiv.org/abs/2109.00122 ",
		"Code repository": "https://github.com/czyssrs/FinQA",
		"Dataset ": "https://huggingface.co/datasets/ibm-research/finqa",
		"Number of examples": "8000",
		"License": "CC-BY-4.0",
		"Initial publication year": "2021",
		"Cited by:": "101-500"
	},
	{
		"Name": "StereoSet",
		"Type": "safety,bias & ethics",
		"Description": "A large-scale natural dataset in English to measure stereotypical biases in four domains: gender, profession, race, and religion.",
		"Benchmark paper": "StereoSet: Measuring stereotypical bias in pretrained language models\nhttps://arxiv.org/abs/2004.09456",
		"Code repository": "https://github.com/moinnadeem/StereoSet",
		"Dataset ": "https://huggingface.co/datasets/McGill-NLP/stereoset",
		"Number of examples": "4229",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2020",
		"Cited by:": ">500"
	},
	{
		"Name": "ETHICS",
		"Type": "safety,bias & ethics",
		"Description": "A set of binary-choice questions on ethics with two actions to choose from.",
		"Benchmark paper": "Aligning AI With Shared Human Values\nhttps://arxiv.org/abs/2008.02275",
		"Code repository": "https://github.com/hendrycks/ethics",
		"Dataset ": "https://huggingface.co/datasets/hendrycks/ethics",
		"Number of examples": "134400",
		"License": "MIT License",
		"Initial publication year": "2020",
		"Cited by:": "101-500"
	},
	{
		"Name": "Social Chemistry 101",
		"Type": "language & reasoning,bias & ethics",
		"Description": "A conceptual formalism to study people\u2019s everyday social norms and moral judgments.",
		"Benchmark paper": "Social Chemistry 101: Learning to Reason about Social and Moral Norms\nhttps://arxiv.org/abs/2011.00620",
		"Code repository": "https://github.com/mbforbes/social-chemistry-101",
		"Dataset ": "https://github.com/mbforbes/social-chemistry-101?tab=readme-ov-file#data",
		"Number of examples": "4500000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2020",
		"Cited by:": "101-500"
	},
	{
		"Name": "RealToxicityPrompt",
		"Type": "safety",
		"Description": "A dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier.",
		"Benchmark paper": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models\nhttps://arxiv.org/abs/2009.11462",
		"Code repository": "https://github.com/allenai/real-toxicity-prompts?tab=readme-ov-file",
		"Dataset ": "https://huggingface.co/datasets/allenai/real-toxicity-prompts",
		"Number of examples": "99442",
		"License": "Apache-2.0 license",
		"Initial publication year": "2020",
		"Cited by:": ">500"
	},
	{
		"Name": "MMLU",
		"Type": "knowledge,language & reasoning",
		"Description": "Multi-choice tasks across 57 subjects, high school to expert level.",
		"Benchmark paper": "Measuring Massive Multitask Language Understanding\nhttps://arxiv.org/abs/2009.03300",
		"Code repository": "https://github.com/hendrycks/test/tree/master",
		"Dataset ": "https://huggingface.co/datasets/cais/mmlu",
		"Number of examples": "231400",
		"License": "MIT License",
		"Initial publication year": "2020",
		"Cited by:": ">500"
	},
	{
		"Name": "CrowS-Pairs (Crowdsourced Stereotype Pairs)",
		"Type": "safety,bias & ethics",
		"Description": "Covers stereotypes dealing with nine types of bias, like race, religion, and age.",
		"Benchmark paper": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models\nhttps://arxiv.org/abs/2010.00133",
		"Code repository": "https://github.com/nyu-mll/crows-pairs",
		"Dataset ": "https://github.com/nyu-mll/crows-pairs/blob/master/data/crows_pairs_anonymized.csv",
		"Number of examples": "1508",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2020",
		"Cited by:": ">500"
	},
	{
		"Name": "MLSUM",
		"Type": "summarization,language & reasoning",
		"Description": "Multilingual summarization dataset crawled from different news websites.",
		"Benchmark paper": "MLSUM: The Multilingual Summarization Corpus\nhttps://arxiv.org/abs/2004.14900",
		"Code repository": "https://github.com/ThomasScialom/MLSUM",
		"Dataset ": "https://huggingface.co/datasets/GEM/mlsum",
		"Number of examples": "535062",
		"License": "see dataset page",
		"Initial publication year": "2020",
		"Cited by:": "101-500"
	},
	{
		"Name": "MedQA",
		"Type": "domain-specific",
		"Description": "Free-form multiple-choice OpenQA dataset for solving medical problems collected from the professional medical board exams.",
		"Benchmark paper": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams\nhttps://arxiv.org/abs/2009.13081",
		"Code repository": "https://github.com/jind11/MedQA",
		"Dataset ": "https://github.com/jind11/MedQA",
		"Number of examples": "12723",
		"License": "MIT License",
		"Initial publication year": "2020",
		"Cited by:": "101-500"
	},
	{
		"Name": "RobustBench",
		"Type": "safety",
		"Description": "Adversarial robustness benchmark.",
		"Benchmark paper": "RobustBench: a standardized adversarial robustness benchmark \nhttps://arxiv.org/abs/2010.09670",
		"Code repository": "https://github.com/RobustBench/robustbench",
		"Dataset ": "see repo",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2020",
		"Cited by:": ">500"
	},
	{
		"Name": "Contrast Sets",
		"Type": "decision-making",
		"Description": "Annotation paradigm for NLP that helps to close systematic gaps in the test data. Contrast sets provide a local view of a model's decision boundary, which can be used to more accurately evaluate a model's true linguistic capabilities.",
		"Benchmark paper": "Evaluating Models' Local Decision Boundaries via Contrast Sets \nhttps://arxiv.org/abs/2004.02709",
		"Code repository": "https://github.com/allenai/contrast-sets",
		"Dataset ": "see repo",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2020",
		"Cited by:": ">500"
	},
	{
		"Name": "Natural Questions",
		"Type": "language & reasoning",
		"Description": "User questions issued to Google search, and answers found from Wikipedia by annotators.",
		"Benchmark paper": "Natural Questions: A Benchmark for Question Answering Research\nhttps://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00276/43518/Natural-Questions-A-Benchmark-for-Question",
		"Code repository": "https://github.com/google-research-datasets/natural-questions",
		"Dataset ": "https://ai.google.com/research/NaturalQuestions",
		"Number of examples": "300000",
		"License": "Apache-2.0 license",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "ANLI",
		"Type": "language & reasoning",
		"Description": "Large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure.",
		"Benchmark paper": "Adversarial NLI: A New Benchmark for Natural Language Understanding\nhttps://arxiv.org/abs/1910.14599",
		"Code repository": "https://github.com/facebookresearch/anli",
		"Dataset ": "https://huggingface.co/datasets/facebook/anli",
		"Number of examples": "169265",
		"License": "CC-BY-NC-4.0",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "SEAT (Sentence Encoder Association Test)",
		"Type": "safety,bias & ethics",
		"Description": "Measures bias in sentence encoders.",
		"Benchmark paper": "On Measuring Social Biases in Sentence Encoders\nhttps://arxiv.org/abs/1903.10561",
		"Code repository": "https://github.com/W4ngatang/sent-bias",
		"Dataset ": "https://github.com/W4ngatang/sent-bias/tree/master/tests",
		"Number of examples": "n/a",
		"License": "CC-BY-NC-4.0",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "BoolQ",
		"Type": "language & reasoning",
		"Description": "Yes/No questions from Google searches, paired with Wikipedia passages.",
		"Benchmark paper": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions\nhttps://arxiv.org/abs/1905.10044",
		"Code repository": "https://github.com/google-research-datasets/boolean-questions",
		"Dataset ": "https://github.com/google-research-datasets/boolean-questions",
		"Number of examples": "16000",
		"License": "CC-BY-SA-3.0",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "SuperGLUE",
		"Type": "language & reasoning",
		"Description": "Improved and more challenging version of GLUE benchmark.",
		"Benchmark paper": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\nhttps://arxiv.org/abs/1905.00537",
		"Code repository": "https://github.com/nyu-mll/jiant",
		"Dataset ": "https://huggingface.co/datasets/aps/super_glue",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "DROP (Discrete Reasoning Over Paragraphs)",
		"Type": "language & reasoning",
		"Description": "Tasks to resolve references in a question and perform discrete operations over them (such as addition, counting, or sorting).",
		"Benchmark paper": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\nhttps://arxiv.org/abs/1903.00161",
		"Code repository": "https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/drop/README.md",
		"Dataset ": "https://huggingface.co/datasets/ucinlp/drop",
		"Number of examples": "96000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "OpenDialKG",
		"Type": "conversation & chatbots",
		"Description": "A dataset of conversations between two crowdsourcing\nagents engaging in a dialog about a given topic.",
		"Benchmark paper": "OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs\nhttps://aclanthology.org/P19-1081/",
		"Code repository": "https://github.com/facebookresearch/opendialkg",
		"Dataset ": "https://github.com/facebookresearch/opendialkg/tree/main/data",
		"Number of examples": "15000",
		"License": "CC-BY-NC-4.0",
		"Initial publication year": "2019",
		"Cited by:": "101-500"
	},
	{
		"Name": "HellaSwag",
		"Type": "language & reasoning",
		"Description": "Predict the most likely ending of a sentence, multiple-choice.",
		"Benchmark paper": "HellaSwag: Can a Machine Really Finish Your Sentence?\nhttps://arxiv.org/abs/1905.07830",
		"Code repository": "https://github.com/rowanz/hellaswag/tree/master",
		"Dataset ": "https://github.com/rowanz/hellaswag/tree/master/data",
		"Number of examples": "59950",
		"License": "MIT License",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "PubMedQA",
		"Type": "domain-specific",
		"Description": "A dataset for biomedical research question answering.",
		"Benchmark paper": "PubMedQA: A Dataset for Biomedical Research Question Answering\nhttps://arxiv.org/abs/1909.06146",
		"Code repository": "https://github.com/pubmedqa/pubmedqa",
		"Dataset ": "https://github.com/pubmedqa/pubmedqa",
		"Number of examples": "270000",
		"License": "MIT License",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "Winogrande",
		"Type": "language & reasoning",
		"Description": "Fill-in-a-blank tasks resolving ambiguities in pronoun references with binary options.",
		"Benchmark paper": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale\nhttps://arxiv.org/abs/1907.10641",
		"Code repository": "https://github.com/allenai/winogrande",
		"Dataset ": "https://huggingface.co/datasets/allenai/winogrande",
		"Number of examples": "44000",
		"License": "Apache-2.0 license",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "PIQA (Physical Interaction QA)",
		"Type": "language & reasoning",
		"Description": "Naive physics reasoning tasks focusing on how we interact with everyday objects in everyday situations.",
		"Benchmark paper": "PIQA: Reasoning about Physical Commonsense in Natural Language\nhttps://arxiv.org/abs/1911.11641",
		"Code repository": "https://github.com/ybisk/ybisk.github.io/tree/master/piqa",
		"Dataset ": "https://huggingface.co/datasets/baber/piqa",
		"Number of examples": "18000",
		"License": "Academic Free License (\"AFL\") v. 3.1",
		"Initial publication year": "2019",
		"Cited by:": ">500",
		"hf_config": "default",
		"note": "Original ybisk/piqa requires Python code execution, using baber/piqa mirror"
	},
	{
		"Name": "Civil Comments",
		"Type": "bias & ethics",
		"Description": "A suite of threshold-agnostic metrics for unintended bias and a test set of online comments with crowd-sourced annotations for identity references.",
		"Benchmark paper": "Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification \nhttps://arxiv.org/abs/1903.04561",
		"Code repository": "https://github.com/conversationai/conversationai.github.io/tree/main",
		"Dataset ": "https://huggingface.co/datasets/google/civil_comments",
		"Number of examples": "1999514",
		"License": "CC0-1.0",
		"Initial publication year": "2019",
		"Cited by:": ">500"
	},
	{
		"Name": "HotpotQA",
		"Type": "language & reasoning",
		"Description": "A set of Wikipedia-based question-answer\npairs with multi-hop questions.",
		"Benchmark paper": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\nhttps://arxiv.org/abs/1809.09600",
		"Code repository": "https://github.com/hotpotqa/hotpot",
		"Dataset ": "https://hotpotqa.github.io/",
		"Number of examples": "113000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "GLUE (General Language Understanding Evaluation)",
		"Type": "language & reasoning",
		"Description": "Tool for evaluating and analyzing the performance of models on NLU tasks. Was quickly outperformed by LLMs and replaced by SuperGLUE.",
		"Benchmark paper": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding\nhttps://arxiv.org/abs/1804.07461",
		"Code repository": "https://github.com/nyu-mll/GLUE-baselines",
		"Dataset ": "https://huggingface.co/datasets/nyu-mll/glue",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "OpenBookQA",
		"Type": "language & reasoning",
		"Description": "Question answering dataset, modeled after open book exams.",
		"Benchmark paper": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering\nhttps://arxiv.org/abs/1809.02789",
		"Code repository": "https://github.com/allenai/OpenBookQA",
		"Dataset ": "https://huggingface.co/datasets/allenai/openbookqa",
		"Number of examples": "12000",
		"License": "Apache-2.0 license",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "WinoGender",
		"Type": "safety,bias & ethics",
		"Description": "Pairs of sentences that differ only by the gender of one pronoun in the sentence, designed to test for the presence of gender bias in automated coreference resolution systems.",
		"Benchmark paper": "Gender Bias in Coreference Resolution\nhttps://arxiv.org/abs/1804.09301",
		"Code repository": "https://github.com/rudinger/winogender-schemas",
		"Dataset ": "https://huggingface.co/datasets/oskarvanderwal/winogender",
		"Number of examples": "720",
		"License": "MIT License",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "CoQA (Conversational Question Answering)",
		"Type": "conversation & chatbots",
		"Description": "Questions with answers collected from 8000+ conversations.",
		"Benchmark paper": "CoQA: A Conversational Question Answering Challenge\nhttps://arxiv.org/abs/1808.07042",
		"Code repository": "https://stanfordnlp.github.io/coqa/",
		"Dataset ": "https://stanfordnlp.github.io/coqa/",
		"Number of examples": "127000",
		"License": "see dataset page",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "SQuAD2.0",
		"Type": "language & reasoning",
		"Description": "Combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.",
		"Benchmark paper": "Know What You Don't Know: Unanswerable Questions for SQuAD\nhttps://arxiv.org/abs/1806.03822",
		"Code repository": "https://rajpurkar.github.io/SQuAD-explorer/",
		"Dataset ": "https://huggingface.co/datasets/bayes-group-diffusion/squad-2.0",
		"Number of examples": "150000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "ARC",
		"Type": "knowledge,language & reasoning",
		"Description": "Grade-school level, multiple-choice science questions.",
		"Benchmark paper": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\nhttps://arxiv.org/abs/1803.05457",
		"Code repository": "https://github.com/allenai/aristo-leaderboard/tree/master/arc",
		"Dataset ": "https://huggingface.co/datasets/allenai/ai2_arc",
		"Number of examples": "7787",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "SWAG",
		"Type": "language & reasoning",
		"Description": "Multi-choice tasks of grounded commonsense inference with adversarial filtering.",
		"Benchmark paper": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\nhttps://arxiv.org/abs/1808.05326\n",
		"Code repository": "https://github.com/rowanz/swagaf",
		"Dataset ": "https://github.com/rowanz/swagaf/tree/master/data",
		"Number of examples": "113000",
		"License": "MIT License",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "CommonsenseQA",
		"Type": "language & reasoning",
		"Description": "Multiple-choice question answering dataset that requires commonsense knowledge to predict the correct answers.",
		"Benchmark paper": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge\nhttps://arxiv.org/abs/1811.00937",
		"Code repository": "https://github.com/jonathanherzig/commonsenseqa",
		"Dataset ": "https://github.com/jonathanherzig/commonsenseqa",
		"Number of examples": "12102",
		"License": "see dataset page",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "QuAC (Question Answering in Context)",
		"Type": "conversation & chatbots",
		"Description": "Question-answer pairs, simulating student-teacher interactions.",
		"Benchmark paper": "QuAC : Question Answering in Context\nhttps://arxiv.org/abs/1808.07036",
		"Code repository": "https://quac.ai/",
		"Dataset ": "https://quac.ai/",
		"Number of examples": "100000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "FEVER",
		"Type": "information retrieval & RAG",
		"Description": "A dataset for verification against textual sources, FEVER: Fact Extraction and VERification.",
		"Benchmark paper": "FEVER: a large-scale dataset for Fact Extraction and VERification \nhttps://arxiv.org/abs/1803.05355",
		"Code repository": "https://github.com/awslabs/fever",
		"Dataset ": "https://fever.ai/dataset/fever.html",
		"Number of examples": "185445",
		"License": "see dataset page",
		"Initial publication year": "2018",
		"Cited by:": ">500"
	},
	{
		"Name": "RACE (ReAding Comprehension Dataset From Examinations)",
		"Type": "language & reasoning",
		"Description": "Reading comprehension tasks collected from the English exams for middle and high school Chinese students.",
		"Benchmark paper": "RACE: Large-scale ReAding Comprehension Dataset From Examinations\nhttps://arxiv.org/abs/1704.04683",
		"Code repository": "n/a",
		"Dataset ": "https://www.cs.cmu.edu/~glai1/data/race/",
		"Number of examples": "100000",
		"License": "see dataset page",
		"Initial publication year": "2017",
		"Cited by:": ">500"
	},
	{
		"Name": "SciQ",
		"Type": "language & reasoning",
		"Description": "Multiple choice science exam questions.",
		"Benchmark paper": "Crowdsourcing Multiple Choice Science Questions\nhttps://arxiv.org/abs/1707.06209",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/allenai/sciq",
		"Number of examples": "13700",
		"License": "CC-BY-SA-3.0",
		"Initial publication year": "2017",
		"Cited by:": "101-500"
	},
	{
		"Name": "TriviaQA",
		"Type": "language & reasoning",
		"Description": "A large-scale question-answering dataset.",
		"Benchmark paper": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\nhttps://arxiv.org/abs/1705.03551",
		"Code repository": "https://github.com/mandarjoshi90/triviaqa",
		"Dataset ": "https://huggingface.co/datasets/mandarjoshi/trivia_qa",
		"Number of examples": "650000",
		"License": "see dataset page",
		"Initial publication year": "2017",
		"Cited by:": ">500"
	},
	{
		"Name": "MultiNLI (Multi-Genre Natural Language Inference)",
		"Type": "language & reasoning",
		"Description": "A crowdsourced collection of sentence pairs annotated with textual entailment information.",
		"Benchmark paper": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\nhttps://arxiv.org/abs/1704.05426",
		"Code repository": "https://github.com/nyu-mll/multiNLI",
		"Dataset ": "https://huggingface.co/datasets/nyu-mll/multi_nli",
		"Number of examples": "433000",
		"License": "see dataset page",
		"Initial publication year": "2017",
		"Cited by:": ">500"
	},
	{
		"Name": "NarrativeQA",
		"Type": "language & reasoning,information retrieval & RAG",
		"Description": "A dataset that requires reading entire books or movie scripts to answer the questions. It requires an understanding of the underlying narrative rather than relying on pattern matching or salience. ",
		"Benchmark paper": "The NarrativeQA Reading Comprehension Challenge \nhttps://arxiv.org/abs/1712.07040 ",
		"Code repository": "https://github.com/google-deepmind/narrativeqa",
		"Dataset ": "https://huggingface.co/datasets/deepmind/narrativeqa_manual",
		"Number of examples": "1572",
		"License": "Apache-2.0 license",
		"Initial publication year": "2017",
		"Cited by:": ">500"
	},
	{
		"Name": "AQUA-RAT",
		"Type": "math",
		"Description": "An algebraic word problem dataset, with multiple choice questions annotated with rationales.",
		"Benchmark paper": "Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems \nhttps://arxiv.org/abs/1705.04146",
		"Code repository": "https://github.com/google-deepmind/AQuA",
		"Dataset ": "https://huggingface.co/datasets/deepmind/aqua_rat",
		"Number of examples": "100000",
		"License": "Apache-2.0 license",
		"Initial publication year": "2017",
		"Cited by:": ">500"
	},
	{
		"Name": "SQuAD (Stanford Question Answering Dataset)",
		"Type": "language & reasoning",
		"Description": "A reading comprehension dataset consisting of 100,000 questions posed by crowdworkers on a set of Wikipedia articles.",
		"Benchmark paper": "SQuAD: 100,000+ Questions for Machine Comprehension of Text\nhttps://arxiv.org/abs/1606.05250",
		"Code repository": "https://rajpurkar.github.io/SQuAD-explorer/",
		"Dataset ": "https://huggingface.co/datasets/rajpurkar/squad",
		"Number of examples": "100000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2016",
		"Cited by:": ">500"
	},
	{
		"Name": "LAMBADA (LAnguage Modelling Broadened to Account for Discourse Aspects)",
		"Type": "language & reasoning",
		"Description": "A set of passages composed of a\ncontext and a target sentence. The task is to guess the last word of the target sentence. ",
		"Benchmark paper": "The LAMBADA dataset: Word prediction requiring a broad discourse context\nhttps://arxiv.org/abs/1606.06031",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/cimec/lambada",
		"Number of examples": "12684",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2016",
		"Cited by:": ">500"
	},
	{
		"Name": "MS MARCO",
		"Type": "language & reasoning",
		"Description": "Questions sampled from Bing's search query logs and passages from web documents.",
		"Benchmark paper": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset\nhttps://arxiv.org/abs/1611.09268",
		"Code repository": "https://microsoft.github.io/msmarco/",
		"Dataset ": "https://huggingface.co/datasets/microsoft/ms_marco",
		"Number of examples": "1112939",
		"License": "see dataset page",
		"Initial publication year": "2016",
		"Cited by:": ">500"
	},
	{
		"Name": "HQHBench",
		"Type": "multimodal",
		"Description": "Evaluates the performance of LVLMs across different types of hallucination. It consists of 4000 free-form VQA image-instruction pairs, with 500 pairs for each hallucination type.",
		"Benchmark paper": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\nhttps://arxiv.org/pdf/1602.07332v1",
		"Code repository": "https://github.com/HQHBench/HQHBench",
		"Dataset ": "see repo",
		"Number of examples": "4000",
		"License": "see dataset page",
		"Initial publication year": "2016",
		"Cited by:": ">500"
	},
	{
		"Name": "NeedleInAHaystack (NIAH)",
		"Type": "information retrieval & RAG",
		"Description": "A simple 'needle in a haystack' analysis to test in-context retrieval ability of long context LLMs.",
		"Benchmark paper": "n/a",
		"Code repository": "https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main",
		"Dataset ": "https://huggingface.co/datasets/YurtsAI/NIAH_eval_dataset",
		"Number of examples": "215",
		"License": "MIT License",
		"Initial publication year": "",
		"Cited by:": ""
	},
	{
		"Name": "MedConceptsQA",
		"Type": "domain-specific",
		"Description": "MedConceptsQA measures the ability of models to interpret and distinguish between medical codes for diagnoses, procedures, and drugs. ",
		"Benchmark paper": "MedConceptsQA: Open source medical concepts QA benchmark \nhttps://www.sciencedirect.com/science/article/pii/S0010482524011740 ",
		"Code repository": "https://github.com/nadavlab/MedConceptsQA",
		"Dataset ": "https://huggingface.co/datasets/ofir408/MedConceptsQA",
		"Number of examples": "819829",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "CUPCase",
		"Type": "domain-specific",
		"Description": "CUPCase is based on 3,563 real-world clinical case reports formulated into diagnoses in open-ended textual format and as multiple-choice options with distractors.",
		"Benchmark paper": "CUPCase: Clinically Uncommon Patient Cases and Diagnoses Dataset \nhttps://arxiv.org/abs/2503.06204",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/ofir408/CupCase",
		"Number of examples": "3562",
		"License": "Apache-2.0 license",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "SPC (Synthetic-Persona-Chat Dataset)",
		"Type": "conversation & chatbots",
		"Description": "A persona-based conversational dataset, consisting of synthetic personas and conversations.",
		"Benchmark paper": "Faithful Persona-based Conversational Dataset Generation with Large Language Models  https://arxiv.org/abs/2312.10007 ",
		"Code repository": "https://github.com/google-research-datasets/Synthetic-Persona-Chat/tree/main ",
		"Dataset ": "https://huggingface.co/datasets/google/Synthetic-Persona-Chat ",
		"Number of examples": "10000+",
		"License": "CC-BY-4.0",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "LAB-Bench (Language Agent Biology Benchmark)",
		"Type": "domain-specific",
		"Description": "An evaluation dataset for AI systems intended to benchmark capabilities foundational to scientific research in biology. ",
		"Benchmark paper": "LAB-Bench: Measuring Capabilities of Language Models for Biology Research \nhttps://arxiv.org/abs/2407.10362 ",
		"Code repository": "https://github.com/Future-House/LAB-Bench",
		"Dataset ": "https://huggingface.co/datasets/futurehouse/lab-bench",
		"Number of examples": "2000",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "PaperBench",
		"Type": "agents & tools use",
		"Description": "A benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. ",
		"Benchmark paper": "PaperBench: Evaluating AI's Ability to Replicate AI Research \nhttps://arxiv.org/abs/2504.01848 ",
		"Code repository": "https://github.com/openai/preparedness/blob/main/project/paperbench/README.md",
		"Dataset ": "https://github.com/openai/preparedness/blob/main/project/paperbench/README.md",
		"Number of examples": "8316",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "Humanity's Last Exam (HLE)",
		"Type": "knowledge,language & reasoning",
		"Description": "A multi-modal benchmark at the frontier of human knowledge, consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences.",
		"Benchmark paper": "Humanity's Last Exam \nhttps://arxiv.org/abs/2501.14249 ",
		"Code repository": "https://github.com/centerforaisafety/hle",
		"Dataset ": "https://huggingface.co/datasets/cais/hle",
		"Number of examples": "2500",
		"License": "MIT License",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "RAFT",
		"Type": "language & reasoning",
		"Description": "A benchmark evaluating the ability of LLMs to solve text classification tasks.",
		"Benchmark paper": "RAFT: A Real-World Few-Shot Text Classification Benchmark\nhttps://arxiv.org/abs/2109.14076",
		"Code repository": "https://github.com/oughtinc/raft-baselines",
		"Dataset ": "https://huggingface.co/datasets/ought/raft",
		"Number of examples": "29000",
		"License": "see dataset page",
		"Initial publication year": "2021",
		"Cited by:": "51-100"
	},
	{
		"Name": "Wildbench",
		"Type": "conversation & chatbots",
		"Description": "An automated evaluation framework designed to benchmark LLMs on real-world user queries. It consists of 1,024 tasks selected from over one million human-chatbot conversation logs. ",
		"Benchmark paper": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild \nhttps://arxiv.org/abs/2406.04770 ",
		"Code repository": "https://github.com/allenai/WildBench",
		"Dataset ": "https://huggingface.co/datasets/allenai/WildBench",
		"Number of examples": "1024",
		"License": "CC-BY-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "LLF-Bench",
		"Type": "agents & tools use",
		"Description": "A benchmark that evaluates the ability of AI agents to interactively learn from natural language feedback and instructions.",
		"Benchmark paper": "LLF-Bench: Benchmark for Interactive Learning from Language Feedback\nhttps://arxiv.org/abs/2312.06853",
		"Code repository": "https://github.com/microsoft/LLF-Bench",
		"Dataset ": "https://github.com/microsoft/LLF-Bench",
		"Number of examples": "n/a",
		"License": "MIT License",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "SocialDial",
		"Type": "conversation & chatbots",
		"Description": "A socially-aware dialogue corpus that covers five categories of social norms, including social relation, context, and social distance.",
		"Benchmark paper": "SocialDial: A Benchmark for Socially-Aware Dialogue Systems \nhttps://arxiv.org/abs/2304.12026",
		"Code repository": "https://github.com/zhanhl316/SocialDial",
		"Dataset ": "https://github.com/zhanhl316/SocialDial/blob/main/human_dialogue_data.json",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "11-50"
	},
	{
		"Name": "We-Math",
		"Type": "math",
		"Description": "A benchmark that evaluates the problem-solving principles in knowledge acquisition and generalization for math tasks.",
		"Benchmark paper": "We-Math: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning? \nhttps://arxiv.org/abs/2407.01284",
		"Code repository": "https://github.com/We-Math/We-Math",
		"Dataset ": "https://huggingface.co/datasets/We-Math/We-Math",
		"Number of examples": "1740",
		"License": "CC-BY-NC-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "ViDoRe (Visual Document Retrieval Benchmark)",
		"Type": "information retrieval & RAG,multimodal",
		"Description": "A benchmark to evaluate LLMs on visually rich document retrieval.",
		"Benchmark paper": "ColPali: Efficient Document Retrieval with Vision Language Models \nhttps://arxiv.org/abs/2407.01449",
		"Code repository": "https://github.com/illuin-tech/vidore-benchmark",
		"Dataset ": "https://huggingface.co/collections/vidore/vidore-benchmark-667173f98e70a1c0fa4db00d",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "CYBERSECEVAL 2",
		"Type": "safety",
		"Description": "A novel benchmark to quantify LLM security risks, including prompt injection and code interpreter abuse.",
		"Benchmark paper": "CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models \nhttps://arxiv.org/abs/2404.13161",
		"Code repository": "https://github.com/meta-llama/PurpleLlama",
		"Dataset ": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/benchmark",
		"Number of examples": "n/a",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "ConfAIde",
		"Type": "safety",
		"Description": "A benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs.",
		"Benchmark paper": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory \nhttps://arxiv.org/abs/2310.17884",
		"Code repository": "https://github.com/skywalker023/confAIde",
		"Dataset ": "https://github.com/skywalker023/confAIde/tree/main/benchmark",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2023",
		"Cited by:": "101-500"
	},
	{
		"Name": "CRAG",
		"Type": "information retrieval & RAG",
		"Description": "A factual question answering benchmark of question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search.",
		"Benchmark paper": "CRAG -- Comprehensive RAG Benchmark \nhttps://arxiv.org/abs/2406.04744",
		"Code repository": "https://github.com/facebookresearch/CRAG",
		"Dataset ": "https://github.com/facebookresearch/CRAG/blob/main/docs/dataset.md",
		"Number of examples": "4409",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "LiveCodeBench ",
		"Type": "coding",
		"Description": "A benchmark that evaluates the coding abilities of LLMs and contains problems from contests across three competition platforms - LeetCode, AtCoder, and CodeForces.",
		"Benchmark paper": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code\nhttps://arxiv.org/abs/2403.07974",
		"Code repository": "https://livecodebench.github.io/",
		"Dataset ": "https://huggingface.co/livecodebench",
		"Number of examples": "1882",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "LiveCodeBench Pro",
		"Type": "coding",
		"Description": "A benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem.",
		"Benchmark paper": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?\nhttps://arxiv.org/abs/2506.11928 ",
		"Code repository": "https://github.com/GavinZhengOI/LiveCodeBench-Pro",
		"Dataset ": "https://huggingface.co/datasets/anonymous1926/anonymous_dataset",
		"Number of examples": "785",
		"License": "MIT License",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "CodeElo",
		"Type": "coding",
		"Description": "A standardized competition-level code generation benchmark.",
		"Benchmark paper": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings \nhttps://arxiv.org/abs/2501.01257",
		"Code repository": "https://github.com/QwenLM/CodeElo",
		"Dataset ": "https://huggingface.co/datasets/Qwen/CodeElo",
		"Number of examples": "408",
		"License": "Apache-2.0 license",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "MM-Vet",
		"Type": "multimodal",
		"Description": "An evaluation benchmark that examines LMMs on complicated multimodal tasks. ",
		"Benchmark paper": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities\n \nhttps://arxiv.org/abs/2308.02490",
		"Code repository": "https://github.com/yuweihao/MM-Vet",
		"Dataset ": "https://huggingface.co/datasets/whyu/mm-vet",
		"Number of examples": "218",
		"License": "CC-BY-NC-4.0",
		"Initial publication year": "2023",
		"Cited by:": ">500"
	},
	{
		"Name": "Livebench",
		"Type": "language & reasoning,coding,math,instruction-following",
		"Description": "A new benchmark designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. It contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets.",
		"Benchmark paper": "LiveBench: A Challenging, Contamination-Limited LLM Benchmark \nhttps://arxiv.org/abs/2406.19314",
		"Code repository": "https://github.com/livebench/livebench",
		"Dataset ": "https://huggingface.co/collections/livebench/livebench-67eaef9bb68b45b17a197a98",
		"Number of examples": "1000+",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "TLDR 9+",
		"Type": "summarization",
		"Description": "A large-scale summarization dataset that contains over 9 million training instances extracted from Reddit discussion forum.",
		"Benchmark paper": "TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts \nhttps://arxiv.org/abs/2110.01159",
		"Code repository": "https://github.com/sajastu/reddit_collector",
		"Dataset ": "https://github.com/sajastu/reddit_collector?tab=readme-ov-file#dataset-links",
		"Number of examples": "9M+",
		"License": "see dataset page",
		"Initial publication year": "2021",
		"Cited by:": "11-50"
	},
	{
		"Name": "LongGenBench",
		"Type": "information retrieval & RAG",
		"Description": "A synthetic benchmark that allows for flexible configurations of customized generation context lengths.",
		"Benchmark paper": "LongGenBench: Long-context Generation Benchmark \nhttps://arxiv.org/abs/2410.04199",
		"Code repository": "https://github.com/mozhu621/LongGenBench",
		"Dataset ": "https://huggingface.co/datasets/mozhu/LongGenBench",
		"Number of examples": "n/a",
		"License": "CC-BY-ND-4.0",
		"Initial publication year": "2024",
		"Cited by:": "11-50"
	},
	{
		"Name": "MultiAgentBench",
		"Type": "agents & tools use",
		"Description": "A comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. It measures task completion and the quality of collaboration and competition.",
		"Benchmark paper": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents\nhttps://arxiv.org/html/2503.01935v1",
		"Code repository": "https://github.com/ulab-uiuc/MARBLE",
		"Dataset ": "https://github.com/MultiagentBench/MARBLE/tree/main/multiagentbench",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "DIBS (Domain Intelligence Benchmark Suite)",
		"Type": "domain-specific,agents & tools use,information retrieval & RAG",
		"Description": "DIBS measures LLM performance on datasets curated to reflect specialized domain knowledge and common enterprise use cases that traditional academic benchmarks often overlook.",
		"Benchmark paper": "Benchmarking Domain Intelligence \nhttps://www.databricks.com/blog/benchmarking-domain-intelligence",
		"Code repository": "n/a",
		"Dataset ": "n/a",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "CRMArena",
		"Type": "agents & tools use",
		"Description": "A benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments. ",
		"Benchmark paper": "CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments\nhttps://arxiv.org/abs/2411.02305",
		"Code repository": "https://github.com/SalesforceAIResearch/CRMArena",
		"Dataset ": "https://huggingface.co/datasets/Salesforce/CRMArena",
		"Number of examples": "1186",
		"License": "CC-BY-NC-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "CRMArena-Pro",
		"Type": "agents & tools use",
		"Description": "A benchmark developed by Salesforce AI Research to evaluate LLM agents in realistic CRM (Customer Relationship Management) tasks",
		"Benchmark paper": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions \nhttps://arxiv.org/abs/2505.18878",
		"Code repository": "https://github.com/SalesforceAIResearch/CRMArena",
		"Dataset ": "https://huggingface.co/datasets/Salesforce/CRMArenaPro",
		"Number of examples": "8614",
		"License": "CC-BY-NC-4.0",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "FutureBench",
		"Type": "agents & tools use",
		"Description": "A benchmarking system that tests agents\u2019 ability to predict real-world outcomes using fresh news and prediction market events.",
		"Benchmark paper": "Back to The Future: Evaluating AI Agents on Predicting Future Events \nhttps://huggingface.co/blog/futurebench",
		"Code repository": "https://huggingface.co/spaces/togethercomputer/FutureBench",
		"Dataset ": "https://huggingface.co/spaces/togethercomputer/FutureBench",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "MegaScience",
		"Type": "knowledge,language & reasoning",
		"Description": "A large-scale mixture of high-quality open-source datasets totaling 1.25 million instances.",
		"Benchmark paper": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning \nhttps://arxiv.org/pdf/2507.16812",
		"Code repository": "https://github.com/GAIR-NLP/MegaScience",
		"Dataset ": "https://huggingface.co/datasets/MegaScience/MegaScience",
		"Number of examples": "1.25M+",
		"License": "CC-BY-NC-SA-4.0",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "TeleMath",
		"Type": "math,domain-specific",
		"Description": "Designed to evaluate LLM performance in solving mathematical problems with numerical solutions in the telecommunications domain.",
		"Benchmark paper": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving\n\nhttps://arxiv.org/abs/2506.10674",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/netop/TeleMath",
		"Number of examples": "500",
		"License": "MIT License",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "Include",
		"Type": "language & reasoning,multilingual",
		"Description": "An evaluation suite to measure the capabilities of multilingual LLMs in a variety of regional contexts across 44 written languages.",
		"Benchmark paper": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge\n\nhttps://arxiv.org/pdf/2411.19799",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/CohereLabs/include-base-44",
		"Number of examples": "22953",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "ResearchCodeBench",
		"Type": "coding",
		"Description": "A benchmark that evaluates LLMs\u2019 ability to translate cutting-edge ML contributions from top 2024-2025 research papers into executable code.",
		"Benchmark paper": "ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code \n\nhttps://arxiv.org/html/2506.02314v1",
		"Code repository": "https://github.com/PatrickHua/ResearchCodeBench",
		"Dataset ": "https://researchcodebench.github.io/leaderboard/index.html",
		"Number of examples": "212",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "HealthBench",
		"Type": "safety,domain-specific",
		"Description": "Realistic healthcare scenarios: emergency referrals, global health, health data tasks, context-seeking, expertise-tailored communication, response depth, and responding under uncertainty.",
		"Benchmark paper": "HealthBench: Evaluating Large Language Models\nTowards Improved Human Health\n\nhttps://cdn.openai.com/pdf/bd7a39d5-9e9f-47b3-903c-8b847ca650c7/healthbench_paper.pdf",
		"Code repository": "https://github.com/openai/simple-evals",
		"Dataset ": "https://github.com/openai/simple-evals",
		"Number of examples": "5000",
		"License": "MIT License",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "FaithEval",
		"Type": "information retrieval & RAG",
		"Description": "Comprehensive evaluation of how well LLMs can align their responses with the context.",
		"Benchmark paper": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If \"The Moon is Made of Marshmallows\" \nhttps://arxiv.org/abs/2410.03727",
		"Code repository": "https://github.com/SalesforceAIResearch/FaithEval",
		"Dataset ": "https://huggingface.co/collections/Salesforce/faitheval-benchmark-66ff102cda291ca0875212d4",
		"Number of examples": "4900",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "PERRECBENCH",
		"Type": "domain-specific",
		"Description": "A novel benchmark for evaluating how well LLMs understand user preferences in recommendation systems.",
		"Benchmark paper": "https://arxiv.org/abs/2501.13391",
		"Code repository": "https://github.com/TamSiuhin/PerRecBench",
		"Dataset ": "https://github.com/TamSiuhin/PerRecBench?tab=readme-ov-file#download-data",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "MediQ",
		"Type": "domain-specific,language & reasoning",
		"Description": "A framework for simulating realistic clinical interactions, where an Expert model asks information-seeking questions when needed and respond reliably. ",
		"Benchmark paper": "MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning\n\nhttps://arxiv.org/abs/2406.00922",
		"Code repository": "https://github.com/stellalisy/mediQ",
		"Dataset ": "https://drive.google.com/drive/folders/1ZPGfr-iftLsQDLkwyNYRg5ERwpuCtLg_",
		"Number of examples": "n/a",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "MTRAG",
		"Type": "information retrieval & RAG",
		"Description": "An end-to-end human-generated multi-turn RAG benchmark that reflects several real-world properties across diverse dimensions for evaluating the full RAG pipeline. ",
		"Benchmark paper": "MTRAG: A Multi-Turn Conversational Benchmark for Evaluating Retrieval-Augmented Generation Systems \n\nhttps://arxiv.org/abs/2501.03468",
		"Code repository": "https://github.com/ibm/mt-rag-benchmark",
		"Dataset ": "https://github.com/ibm/mt-rag-benchmark?tab=readme-ov-file#human-data",
		"Number of examples": "842",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "ContextualBench",
		"Type": "information retrieval & RAG",
		"Description": "A compilation of 7 popular contextual question answering benchmarks to evaluate LLMs in RAG application.",
		"Benchmark paper": "n/a",
		"Code repository": "https://github.com/SalesforceAIResearch/SFR-RAG/blob/main/README_ContextualBench.md",
		"Dataset ": "https://huggingface.co/datasets/Salesforce/ContextualBench",
		"Number of examples": "215527",
		"License": "see dataset page",
		"Initial publication year": "",
		"Cited by:": "New"
	},
	{
		"Name": "SpreadsheetBench",
		"Type": "agents & tools use",
		"Description": "A challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios.",
		"Benchmark paper": "SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation\n\nhttps://arxiv.org/abs/2406.14991",
		"Code repository": "https://github.com/RUCKBReasoning/SpreadsheetBench",
		"Dataset ": "https://github.com/RUCKBReasoning/SpreadsheetBench/tree/main/data",
		"Number of examples": "912",
		"License": "CC-BY-SA-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "TheAgentCompany",
		"Type": "agents & tools use",
		"Description": "An extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. ",
		"Benchmark paper": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks \n\nhttps://arxiv.org/abs/2412.14161",
		"Code repository": "https://github.com/TheAgentCompany/TheAgentCompany",
		"Dataset ": "https://github.com/TheAgentCompany/TheAgentCompany/blob/main/workspaces/README.md",
		"Number of examples": "175",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "MultiNRC",
		"Type": "language & reasoning,multilingual",
		"Description": "Assesses LLMs on reasoning questions written by native speakers in French, Spanish, and Chinese. MultiNRC covers four core reasoning categories: language-specific linguistic reasoning, wordplay & riddles, cultural/tradition reasoning, and math reasoning with cultural relevance. ",
		"Benchmark paper": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs \n\nhttps://arxiv.org/abs/2507.17476",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/ScaleAI/MultiNRC",
		"Number of examples": "1000",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "SKA-Bench",
		"Type": "knowledge,language & reasoning",
		"Description": "A Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: Knowledge Graph (KG), Table, KG+Text, and Table+Text.",
		"Benchmark paper": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs \n\nhttps://arxiv.org/abs/2507.17178",
		"Code repository": "https://github.com/Lza12a/SKA-Bench",
		"Dataset ": "https://github.com/Lza12a/SKA-Bench",
		"Number of examples": "2100",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "TaxCalcBench",
		"Type": "agents & tools use,domain-specific",
		"Description": "A benchmark for determining models' abilities to calculate personal income tax returns given all of the necessary information. ",
		"Benchmark paper": "TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task \n\nhttps://arxiv.org/abs/2507.16126",
		"Code repository": "https://github.com/column-tax/tax-calc-bench",
		"Dataset ": "https://github.com/column-tax/tax-calc-bench?tab=readme-ov-file#the-taxcalcbench-eval-ty24-dataset",
		"Number of examples": "51",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "WixQA",
		"Type": "information retrieval & RAG",
		"Description": "A benchmark suite featuring QA datasets grounded in the released knowledge base corpus, enabling holistic evaluation of retrieval and generation components. ",
		"Benchmark paper": "https://arxiv.org/abs/2505.08643 \nWixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/Wix/WixQA",
		"Number of examples": "12842",
		"License": "MIT License",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "SciGym",
		"Type": "agents & tools use,domain-specific",
		"Description": "A benchmark that assesses LLMs\u2019 iterative experiment design and analysis abilities in open-ended scientific discovery tasks. It challenges models to uncover biological mechanisms by designing and interpreting simulated experiments.",
		"Benchmark paper": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab\nhttps://arxiv.org/html/2507.02083v1 ",
		"Code repository": "https://github.com/h4duan/SciGym",
		"Dataset ": "https://huggingface.co/datasets/h4duan/scigym-sbml",
		"Number of examples": "350",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "DSBench",
		"Type": "agents & tools use",
		"Description": "DSBench evaluates large language and vision-language models on realistic data science tasks, including data analysis and data modeling tasks.",
		"Benchmark paper": "DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?\nhttps://arxiv.org/abs/2409.07703",
		"Code repository": "https://github.com/LiqiangJing/DSBench",
		"Dataset ": "https://github.com/LiqiangJing/DSBench?tab=readme-ov-file#usage",
		"Number of examples": "540",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Spider 2.0",
		"Type": "coding",
		"Description": "An evaluation framework comprising real-world text-to-SQL workflow problems derived from enterprise-level database use cases. ",
		"Benchmark paper": "Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows\nhttps://arxiv.org/abs/2411.07763",
		"Code repository": "https://github.com/xlang-ai/Spider2",
		"Dataset ": "https://github.com/xlang-ai/Spider2?tab=readme-ov-file#data",
		"Number of examples": "632",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "BrowseComp",
		"Type": "agents & tools use",
		"Description": "A benchmark for measuring the ability of AI agents to browse the web. Comprises of questions that require persistently navigating the internet in search of hard-to-find, entangled information. ",
		"Benchmark paper": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents\nhttps://arxiv.org/abs/2504.12516",
		"Code repository": "https://github.com/openai/simple-evals",
		"Dataset ": "https://github.com/openai/simple-evals",
		"Number of examples": "1266",
		"License": "MIT License",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "MathArena",
		"Type": "math",
		"Description": "A benchmark for evaluating LLMs on newly-released math competition problems.",
		"Benchmark paper": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions\nhttps://arxiv.org/abs/2505.23281 ",
		"Code repository": "https://github.com/eth-sri/matharena",
		"Dataset ": "https://github.com/eth-sri/matharena",
		"Number of examples": "149",
		"License": "see dataset page",
		"Initial publication year": "2025",
		"Cited by:": "New"
	},
	{
		"Name": "MLE-bench",
		"Type": "agents & tools use",
		"Description": "A benchmark for measuring how well AI agents perform at machine learning engineering.",
		"Benchmark paper": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering\nhttps://arxiv.org/abs/2410.07095 \n",
		"Code repository": "https://github.com/openai/mle-bench",
		"Dataset ": "https://github.com/openai/mle-bench",
		"Number of examples": "75",
		"License": "see dataset page",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "SciCode",
		"Type": "coding",
		"Description": "A benchmark that challenges language models to code solutions for scientific problems.",
		"Benchmark paper": "SciCode: A Research Coding Benchmark Curated by Scientists\nhttps://arxiv.org/abs/2407.13168 ",
		"Code repository": "https://github.com/scicode-bench/SciCode",
		"Dataset ": "https://huggingface.co/datasets/SciCode1/SciCode",
		"Number of examples": "80",
		"License": "Apache-2.0 license",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "DebateSum",
		"Type": "argumentation",
		"Description": "A large-scale argument mining and summarization dataset containing 187,386 debate documents with arguments, extractive summaries, and citations from the National Speech and Debate Association.",
		"Benchmark paper": "DebateSum: A large-scale argument mining and summarization dataset\nhttps://arxiv.org/abs/2011.07251",
		"Code repository": "https://github.com/Hellisotherpeople/DebateSum",
		"Dataset ": "https://huggingface.co/datasets/Hellisotherpeople/DebateSum",
		"Number of examples": "240566",
		"License": "MIT License",
		"Initial publication year": "2020",
		"Cited by:": "51-100"
	},
	{
		"Name": "Argument Quality (IBM)",
		"Type": "argumentation",
		"Description": "A dataset of 30,497 crowd-sourced arguments for 71 debatable topics, labeled for argument quality and stance. Part of IBM Project Debater.",
		"Benchmark paper": "A Large-scale Dataset for Argument Quality Ranking: Construction and Analysis\nhttps://arxiv.org/abs/1911.11408",
		"Code repository": "https://github.com/IBM/debater-eap-tutorial",
		"Dataset ": "https://huggingface.co/datasets/ibm-research/argument_quality_ranking_30k",
		"Number of examples": "30497",
		"License": "CC-BY-SA-3.0",
		"Initial publication year": "2019",
		"Cited by:": "101-500"
	},
	{
		"Name": "Claim Stance (IBM)",
		"Type": "argumentation",
		"Description": "A dataset for claim stance classification with claims annotated for their stance (PRO/CON) towards controversial topics. Part of IBM Project Debater.",
		"Benchmark paper": "A Dataset of General-Purpose Rebuttal\nhttps://arxiv.org/abs/1909.00393",
		"Code repository": "https://research.ibm.com/haifa/dept/vst/debating_data.shtml",
		"Dataset ": "https://huggingface.co/datasets/ibm-research/claim_stance",
		"Number of examples": "1355",
		"License": "CC-BY-SA-3.0",
		"Initial publication year": "2019",
		"Cited by:": "51-100"
	},
	{
		"Name": "Logical Fallacy",
		"Type": "argumentation",
		"Description": "A dataset for logical fallacy detection containing statements annotated with 13 types of logical fallacies across educational and climate domains.",
		"Benchmark paper": "Logical Fallacy Detection\nhttps://arxiv.org/abs/2202.13758",
		"Code repository": "https://github.com/causalNLP/logical-fallacy",
		"Dataset ": "https://huggingface.co/datasets/tasksource/logical-fallacy",
		"Number of examples": "2449",
		"License": "MIT License",
		"Initial publication year": "2022",
		"Cited by:": "51-100"
	},
	{
		"Name": "OpenDebateEvidence",
		"Type": "argumentation",
		"Description": "The largest argument mining dataset with 3.5M+ debate evidence documents from NDCA/NDT policy debates (2002-2022). Includes hierarchical summaries, citations, and rich tournament metadata.",
		"Benchmark paper": "OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset\nhttps://arxiv.org/abs/2406.14657",
		"Code repository": "https://github.com/Hellisotherpeople/OpenDebateEvidence",
		"Dataset ": "https://huggingface.co/datasets/Yusuf5/OpenCaselist",
		"Number of examples": "465187",
		"License": "MIT License",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "Persuasion (Anthropic)",
		"Type": "argumentation",
		"Description": "A dataset of claims with human-written and model-generated arguments, annotated with persuasiveness scores measuring how effectively arguments change opinions.",
		"Benchmark paper": "Measuring the Persuasiveness of Language Models\nhttps://arxiv.org/abs/2406.17753",
		"Code repository": "n/a",
		"Dataset ": "https://huggingface.co/datasets/Anthropic/persuasion",
		"Number of examples": "3939",
		"License": "CC-BY-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	},
	{
		"Name": "args.me Corpus",
		"Type": "argumentation",
		"Description": "A corpus of 382,545 arguments crawled from four debate portals: Debatewise, IDebate.org, Debatepedia, and Debate.org. Arguments are structured with premises and conclusions.",
		"Benchmark paper": "args.me: A Corpus of Arguments and Counter-Arguments\nhttps://arxiv.org/abs/1911.11408",
		"Code repository": "https://github.com/webis-de/args.me",
		"Dataset ": "https://huggingface.co/datasets/webis/args_me",
		"Number of examples": "382545",
		"License": "CC-BY-4.0",
		"Initial publication year": "2019",
		"Cited by:": "101-500"
	},
	{
		"Name": "EU Parliament Debates",
		"Type": "argumentation",
		"Description": "A corpus of 87k parliamentary speeches from the European Parliament (2009-2023). Contains political debate transcripts with speaker and party metadata.",
		"Benchmark paper": "n/a",
		"Code repository": "https://github.com/coastalcph/eu_debates",
		"Dataset ": "https://huggingface.co/datasets/coastalcph/eu_debates",
		"Number of examples": "87000",
		"License": "CC-BY-4.0",
		"Initial publication year": "2024",
		"Cited by:": "New"
	}
]
