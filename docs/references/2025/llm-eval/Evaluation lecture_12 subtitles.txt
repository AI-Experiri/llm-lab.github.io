Let's get started.
Today, we're going to
talk about evaluation.
This is one of these topics
that I think looks simple
but actually is far from it.
Mechanically, it's just
given a fixed model.
Ask the question,
how good is it?
So seems pretty easy enough.
And if you think
about evaluation,
you probably see
a lot of things,
such as benchmark scores.
So for example, papers that
put out language models,
put out some benchmarks,
scores on various benchmarks
like MMLU, Amy Codeforces.
Here's a Llama 4 paper.
They evaluate on MMLU Pro,
MATH 500, GPQA, at least
for language.
And then there's some
multimodal stuff.
If you look at OLMo,
it's kind of MATH MMLU
and then some other things
like DROP and GSM8k, and so on.
And see all these numbers.
Most language
models are evaluated
on roughly the same
benchmarks, but not quite.
But what are these
benchmarks, and what
do these numbers actually mean?
So here is another
example from HELM,
where we have a bunch of
different standard benchmarks
which are all collated
together, which
is something we'll talk
about a little bit later.
There's also benchmarks that
look at the costs, not just
the accuracy score.
So Artificial Analysis is this
website that does, I think,
a fairly good job of looking
at these Pareto frontiers
where they have this
intelligence index, which
is basically a combination of
different benchmarks and then
the price that you would have to
pay per token to use that model.
And of course, O3
is really good,
but it's also really expensive.
And apparently, I guess
some of these other models,
actually, according
to this index,
are at least as good and
much cheaper, it seems.
And maybe another
way to look at it
is a model is good if
people choose to use it.
So Open Router is
this website that
essentially has
traffic that gets
routed to a bunch of models.
So they have data on which
models people are choosing.
And so if you just look
at the number of tokens
that are sent to each model,
you can define a leaderboard,
and you can take a leap
of faith and assume
that people are choosing
the models that are good.
So according to this, then
OpenAI, Anthropic, and Google
seem to be at the top.
Here's another
one, Chatbot Arena,
which I think is very popular.
I'll talk a little
bit more about this,
but yeah, it's another
ranking between models
where people on the
internet have conversations
with these models and express
their pairwise preferences.
So there's a lot of
numbers and rankings
that I'm just kind
of throwing at you.
And then you see these vibes
where people post on X. Hey,
look at this awesome example
of something the language model
can do.
There's a lot of these
examples out there.
So that's another source of
data on how good models are.
But really I think
Andrej Karpathy
did a good job of assessing
the current situation, which
is that there is an
evaluation crisis.
There are some benchmarks
like MMLU, which apparently
were good to look at.
But now, the
underlying assumption
is that maybe they have been
either saturated or gamed
or something in between.
And then there's problems with
the Chatbot Arena, which we'll
talk about a little bit later.
And so really, we
have all these models.
We have this plethora of
benchmarks and numbers
that are coming out,
and it's unclear,
I think, at this point,
which are the right way
to do evaluation.
You notice a pattern
in this class where
everything is kind of messy,
and evaluation is no different.
So in this class, I want to
talk a little bit about how you
wish to think about evaluation.
And then I'm going to go through
a bunch of different benchmarks
and talk about a few
issues with benchmarks.
So evaluation, at some level,
is just a mechanical process.
You take an existing model.
You don't really worry
about how it was trained.
And then you throw
prompts at it.
You get some responses.
You compute some metrics,
and you average the numbers.
So it seems like a kind of a
quick script that you can write.
But actually, evaluation is
really kind of a profound topic.
And it also determines
how language models are
going to be built, because
people build these evaluations,
and the top language
model developers
are tracking these over time.
And if you track
something and you're
trying to get your
number to go up,
it's going to really
influence the way
that you develop your model.
So that's why
evaluation, I think,
is really maybe a leading
indicator of where
things are going to go.
So what's the point
of evaluation?
Why do we even do it?
So the answer is that there
is no one true evaluation.
It depends on what question
you're trying to answer.
And this is an important
point because there's
no such thing as, oh, I'm
just evaluating a model.
You get a number.
But what does that
number tell you?
And does it actually answer
your original question?
So here are some examples of
what you might want to do.
So suppose you're a
user or a company,
and you're trying to
make a purchase decision.
So you can use either use
cloud or you can use Grok
or you can use Gemini or O3.
And which one should you choose
for your particular use case?
Another is that your
researcher, you're
not actually trying to use
the model for anything.
You just want to know what
are the raw capabilities
of the model?
Are we making scientific
progress on an AI?
So that's a much
more general question
that's not anchored to
any particular use case.
And then policymakers
and businesses
might want to just
understand objectively,
at a given point in time, what
are the benefits and harms
of a model?
Where are we?
Are models giving us--
telling us the right answer?
How are they helping?
How much value are
they delivering?
Model developers might
be doing evaluation
because they want to get
feedback to improve the model.
They might evaluate and see,
oh, this score is too low.
So let's try an intervention.
And it goes up.
Therefore, we keep
the intervention.
Evaluation is often used
in the development cycle
of language models as well.
So in each case,
there is some goal
that the evaluator
wants to achieve,
and this needs to be translated
into a concrete evaluation.
And the concrete
evaluation you choose
will depend on what
you're trying to achieve.
So in evaluation, there's--
here's a simple framework you
can think about.
So what are the
inputs, the prompts?
How do you call
the language model?
And then once a language
model produces outputs,
how do you assess the outputs?
And then how do you
interpret the results?
So let's look at each
of these questions.
So the inputs-- so where do
you get the set of prompts?
Which use cases are covered
by-- by your prompts?
That's a question.
Do they have representation
of the tails?
Do they have difficult inputs
that challenge the model,
or are they vanilla easy
cases that any language
model would be able to do?
And then finally, in the
multi-turn chatbot setting,
the inputs are actually
dependent on the model.
So that introduces
a complication.
And even in the
single-term setting,
you might be wanting to
choose inputs that are
tailored to the model as well.
So the question of inputs.
And then how do you
call a language model?
So there's many ways to
prompt a language model.
You can do few shot, zero
shot, chain of thought.
And we'll see that
each of these decisions
actually introduces
a lot of variance
into how the valuation metric.
So language models are still
very sensitive to the prompt,
which means that evaluation
needs to take that into account.
And the particular type
of strategy you're using
is something that
you have to decide,
whether you have tool
use for arithmetic,
or you're able to do RAG if--
or use a tool if you are doing
some of recent knowledge query.
And then finally, I think
we'll talk about agents
in a-- a little bit later.
Are we even evaluating?
What is the object
of evaluation?
Are we evaluating
a language model,
or are we evaluating
the whole system?
And this is also an
important distinction
because the model
developer might
want to evaluate the
former because they're
trying to make their
language model better,
and the agentic system
and the scaffolding
is just a means to
derive the metric.
But the user doesn't
care what you're doing
with what language model using.
There might be multiple
language model.
They just care about
the system as a whole.
And then finally, the outputs.
How do you evaluate outputs?
Often, you have
reference outputs.
And are these clean?
Are they error-free?
Very basic question,
but we'll see later
that that's not
obviously the case.
What metrics do you use
for code generation?
Is it pass at one?
Is it pass at 10?
How do you factor into the cost?
Because you see a lot
of the leaderboards,
they're completely--
the cost is kind of
marginalized away.
So you don't have a sense of
maybe the top model is actually
10 times more expensive than
the second model, for example.
And that's why Pareto frontiers
are generally good to look at.
And obviously, in
some use cases,
not all errors
are created equal.
And how do you incorporate that
into your evaluation criteria?
And opening a
generation is obviously
tricky to evaluate because
there's no ground truth.
[? During ?] some text,
write me a compelling story
about Stanford.
That's how you evaluate that.
So suppose you get
through all those.
Now you have the metrics.
And how do you interpret it?
So suppose you get a 91 number.
Does that mean it's good?
If you're a company, do you
deploy it to your users?
Is that good enough?
How do you determine if you're--
let's say, you're a researcher--
has this language model really
learned particular types
of generalization?
This requires us to confront
the issue of train test overlap.
And then finally, we'll talk
a little bit about how--
again, what is the
object of the evaluation?
Is it the model or the system,
or is it actually the method?
So often, in research, the
output of the research paper
is a new method for
doing something.
It's not necessarily the model.
The model is just an example
application of a method.
So if you're
evaluating the method,
then I think many of
the actual evaluations
that people do don't
really make sense
unless you have clear
controls on what you're doing.
So in summary, there's
a lot of questions
to actually think through when
you're doing an evaluation.
It's not just take
a bunch of prompts
and feed it into
a language model.
Yeah, question.
A question for inputs.
It said that are the inputs
adapted to the model.
So should they be adapted,
or shouldn't they be adapted?
So the question is, should the
inputs be adapted to the model?
Again, this depends on
what you're trying to do.
So in some cases,
like the multi-turn,
they have to be
adapted to the model.
I think it's not realistic
to have a static chatbot
evaluation where you have user
assistant, user assistant,
but the assistant
is someone else.
And you're meant to
respond because you
might be put in a
kind of a weird spot
that you would never
get into if you
were driving the conversation.
In red teaming, it's helpful
to adapt the evaluation
to the model because you're
looking for these very rare tail
events, and you're
just going to be
very inefficient if you're just
generically generating prompts.
But of course, when you adapt
your evaluation to the model,
now how do you compare it
between different models?
So there's a trade-off there.
Have any other
questions on this kind
of broad kind of
conceptual level before we
dive into details?
Yeah.
I think it's something
that we've relied on so far
is that perplexity seems to
be informative about a lot
of capabilities that models
improve, all these capabilities.
And I'm curious if in the
natural language setting,
are there any sets of
these questions that
don't have that strong
relationship that
don't seem to be improving
well as we improve perplexity?
Or does that somewhat
genuinely [INAUDIBLE]
enough to convince yourself
that you're improving?
Yeah, so the question is,
is perplexity all you need,
or are there some things that
aren't captured by perplexity?
So that's actually a good
segue to talk about perplexity.
But to answer your
question more directly,
so Tatsu showed a
slide, I think last--
maybe last lecture
that was looking
at the correlation between
perplexity and downstream task
performance.
And it was all over the place,
at least in that setting.
So it's not always the case
that perplexity is correlated
with the thing you care about.
That said, I think what has been
shown is that over long enough
time, like over multiple
scales, perplexity does globally
correspond to everything
improving because the stronger
models are strong
at most things,
and the small 1B models are just
worse on most things overall.
And yeah, so maybe I'll say
a bit more about perplexity.
So remember that a language
model is a distribution
over sequences of tokens.
Perplexity measures essentially
whether the language model
is assigning high
probability to some data set,
so you can define the perplexity
against a particular data
set, usually some sort
of validation set.
So in pre-training, we're
minimizing the perplexity
of the training set.
So the natural thing is when
you're evaluating a language
model, you want to evaluate
the perplexity on the test set.
The standard thing is
having an IED split.
So and this is indeed how
language modeling research
was in the last decade.
So in the 2010s, there were
various standard data sets
for language modeling.
So there's a Penn Treebank,
which actually goes back
to the '90s Wikitext, a
1 billion word benchmark,
which came from
machine translation.
And it has a lot of translated
government proceedings and news.
And so these are the data
sets that people used.
And generally,
what you did was--
I have-- I'm an LLM researcher.
I'd try and pick one of these.
I pick Wall Street Journal.
I train on the designated
training split,
and I evaluate on
Wall Street Journal
the designated test split.
And I look at the accuracy.
There's a bunch of
work in the 2010s.
This was the transition
between n-gram models.
And then there was people
mixing in neural with n-gram,
and there's all sorts of things.
And I think that one of the--
the most prominent results
in the mid-2010s was this
paper from Google that
showed if you design
the architecture right,
you can actually--
and scale up, you
can actually dramatically
reduce the perplexity.
So if you think about
51 to 30, that's
like a massive
perplexity reduction.
And so to go back to what
questions you're asking,
the perplexity of
this game was really
helpful for advancing
language modeling research
because it was a
challenge problem.
One of the points
in this paper was
that on the smaller data
sets, people were worried
about overfitting and all that.
In the larger data sets, you
just have a different game.
The game was to even
just fit the data at all.
And then GPT-1, GPT-2,
I think, changed the way
that people viewed perplexity
or language model evaluations.
So remember, GPT-2 trained
on 40 gigabytes of text.
These were websites that
were linked from Reddit.
And then you just
evaluate directly on--
no fine tuning, directly on the
standard perplexity benchmarks.
So this is clearly out of
distribution evaluation.
You're training a web
text, and then you're
going to evaluate on wiki text.
But the point is that the
training is broad enough--
web text is broad enough--
that you hope that you
get strong generalization.
So they showed a
[INAUDIBLE] table
like this, where you have
different sizes of the models,
and you have
different benchmarks.
So here they have
the Penn Treebank,
and you have wiki text.
And you have one billion words.
And you're looking
at the perplexity
on all these benchmarks.
And at least on the
small data sets,
such as Penn Treebank,
which is kind of tiny,
they were actually able to get
beyond the state of the art.
So they didn't train on
Penn Treebank at all.
And they were able to--
because they trained
on so much other data,
they were able to beat
this state of the art on that.
Now, for one billion
words, they were still on--
above by quite a
bit because once you
have a large enough
data set, then
just train it directly
on that data set
is going to be better than
trying to rely on transfer,
at least at some
1 billion scale.
Yeah.
If they're trained on
websites from Reddit,
how do you know you're not
including like Penn Treebank?
[INAUDIBLE]
Yeah, so the question is, if
you're training on wet data,
how do you know you're not
just training on Penn Treebank?
So this is a huge
issue, in general.
Train tests overlap with
train test contamination.
We'll talk about it a bit later.
Typically, people just
do the decontamination.
So they take their test set,
and they remove any document
or paragraph or whatever that
has a 13-gram overlap with
the test set.
Now there's subtleties
there because there
might be like slight
paraphrases that still
might be-- like near
duplicates don't get detected,
and it's messy.
There's also even cases where
you might get math problems that
are translating into another
language which have no overlap
but still are--
essentially, if you
have the answer,
language models are good
enough that they can translate
in their heads false positives.
It's one thing
supporting another?
If you have also tons
of false positives,
if you have training sets
that quote the test set.
Yeah.
Yeah.
So that generally
is-- oh, I mean,
it's better to be
conservative here
because there's
so much web text.
If you didn't train on some
core text and you still do,
well, then I think that's fine.
You just don't want to
overpromise your model
performance here.
Yeah, so that's something
we'll come back to.
Yeah.
[INAUDIBLE] train on
the smaller data set,
we can train on a
large data set is still
in large part [INAUDIBLE]
a small problem.
So the question is, can
you distill a large model
into a smaller model?
Instead of the train the
large model on a small set--
Yeah.
--we train the large
data set [INAUDIBLE]
still model performance.
So here the model size
isn't really something
that we're too worried about.
You get to choose
any model size.
In fact, I think compute
budget isn't really
the standard size here.
It's just more about
data efficiency.
You're given this data set.
Can you get the best perplexity
on these standard data sets?
Yeah, so this is the
shift of what it means
to evaluate language models.
And then since GPT-2
and GPT-3 language
modeling papers have shifted
more towards downstream task
accuracy-- so most
of this lecture
is going to be
about some of task,
but I want to put in a
plug for perplexity still.
So perplexity, I
think, is still useful
because for several
reasons, it's
smoother than
downstream task accuracy
because you're getting all
these fine-grained logits
and probabilities
of individual tokens
rather than just I
generated some stuff,
and is it correct or wrong?
And it turns out that all the
scaling stuff is done generally
with perplexity of some
sort because it allows
you to more gracefully fit these
curves rather than otherwise,
you get these kind
of discontinuities.
And it won't be quite linear.
The other thing is, which I'll
talk a little bit later about,
is perplexity.
In some senses, you're
universal in the sense
that you're paying
attention to every token
that you have a data set.
You're only paying
attention to every token.
Whereas task accuracy, you
might miss some nuances.
In particular, you can
get an answer correct,
but for the wrong
reasons, especially
if your data set is gameable.
Now note that perplexity is
still useful even in down task--
downstream tasks as well,
because you can essentially
condition on the prompt
and look at the probability
of the answer.
So there's some Scala
papers that do this.
So instead of relying
just on validation loss
on some sort of
corpus, they look
at downstream tasks, which
they care about and fit
scaling laws directly for that.
So one caveat about
perplexity-- and this
is from the
perspective-- suppose
you're running a leaderboard.
People are submitting
their models,
and you want to report
their perplexities.
Now there's a sort
of a dilemma here
because you need to
trust the language model
provider to some extent.
So if you're just
doing task accuracy,
you just take the model.
You run it, and then you
get the generate output.
And then now you
have your code that
evaluates a generated output
against the reference.
And it could be exact match.
It could be F1.
It could be something else.
And then you're fine.
So you don't really need to
look inside the black box.
But for perplexity, remember
the language model has
to generate
probabilities, and you
have to trust that they
are going to sum to 1,
So if you expose an
interface, which is give me
the probability
of this sequence,
then if they not
even maliciously.
They might just have a
bug where they assign
probability 0.8 to everything.
And then they're going to look
really good, except for that's
not a valid distribution.
So that's just one
kind of caveat.
And Perplexity evaluations
are very easy to screw up
if you're not careful.
Yeah, question.
How can it be that you're
generating probabilities
like all of these [INAUDIBLE]?
So the question is,
how can you generate
probabilities that are all 0.8?
That's if you have
a bug, for example.
I think it gets tricky.
So for autoregressive
models, if your interface
is like you have to give me
the logits of all the words,
then I can verify myself
that they sum to 1.
But if I'm just
giving you, let's say,
the probability
of the next token
and you say 0.8, well, because
I'm giving you the token,
I don't have a way of verifying
that all the other tokens need
to sum to 1.
Yeah.
Just going to ask if
it's not [INAUDIBLE]
The question is, is it
standard to get all the logits?
So usually, if you're
computing perplexity,
you have fairly deep access,
and you're just like computing.
And you look at the code,
and you make sure it's right.
But you do have to double-check.
Yeah.
OK.
So here on this
point about universe.
So there are some
people in the world who
we call perplexity maximalists.
And their view is as follows.
So let's say your true
distribution is t,
and your model is p.
So the true distribution,
imagine it's
like this wonderful thing.
You have a prompt, and it
just magically gives you
the right answer and so on.
And so in that case,
the best perplexity
you can get from a
model is kind of lower
bounded by entropy of t.
And that's exactly
when p equals t.
So this is basically
distribution matching.
So by basically minimizing
the perplexity of P
with respect to t,
you're basically
forcing p to be as
close to t as possible.
And in the limit, if you have
T, then you solve all the tasks.
And you reach AGI,
and then you're done.
The counter to this
is that this might not
be the most efficient
way to get there
because you might
be pushing down
on parts of the distribution
that just don't matter.
There's a reason we
define these tasks
in a certain way, because we
are curating what we care about,
rather than just
blindly matching
the probability of every single
token, which is something
that I think clearly
humans don't have to do.
But nonetheless,
perplexity-- or I
guess minimization has
been tremendously useful
for training.
And there's something I think
to this about evaluation
as well, especially
in light of how
benchmarks have been gameable.
In some ways, like
perplexity, as long
as your train and
test are separate,
it is not really a kind
of a gameable quantity.
OK, just to mention a few other
things that look like perplexity
but aren't perplexity.
So there's close
tasks where the idea
is that you get some
sentence, and you're
meant to complete the
filling in the missing word.
So lambada is a task this,
where the context is chosen
to be particularly
challenging, and you
need to look at long contexts.
And you're supposed
to guess the word.
So this has been
kind of saturated.
So a lot of the tasks
that look like perplexity
have just been really
obliterated by a language model
because they're
basically perplexity.
Here's another one.
Hella swag where you--
it's trying to get at
common sense reasoning.
You have a sentence,
and you're trying
to pick the completion
that makes the most sense.
So this is essentially
the way you evaluate
is that you look at the
probability of each candidate
given the prompt, and you're
just measuring the likelihood.
There's some wrinkle
with the normalizing
over the number of tokens.
But more or less, this
is about perplexity.
Yeah.
So what is the goal
of the video here?
What do you use to train
the data [INAUDIBLE]?
Yeah, so the question is, what
is the role of the video here?
Ignore that.
The data is completely all text.
So the way that the
data was created
was to use ActivityNet and
then wiki how to mine the data.
Yeah, yeah.
Actually, this is kind of--
brings me to this other point
about--
that's already been mentioned
about train task overlap, which
is WikiHow as a website.
And while there was
a bunch of processing
that happened to generate this
exact question from WikiHow,
if you go to WikiHow,
you'll see things
that look very much like the
hella swag training set, even--
or [? the ?] data set,
even though it's not
like a kind of a verbatim match.
So you have to be
very, very careful.
OK, so now let me go through
some standard knowledge, or just
benchmarks, that are popular
for evaluating language models.
And for each one, I just
want to describe it--
I think talking about
where the data comes from,
where the state of
the art is, and so on.
So MMLU, which is probably the
canonical standardized test
for language models by now,
that's actually quite old.
It's from 2020.
This was right after
GPT-3 came out.
And at the time, this
was a little bit pretty--
I think it was pretty
forward-looking because
at that time, the idea of
having a language model that
could zero shot, or even few
shot, a ton of different things,
was wild.
How would you get a
language model just
to solve all these
questions automatically?
But now it seems like, oh,
yeah. yeah, you just put it
into ChatGPT, and it works.
But at that time,
it was not obvious.
So what they did was
they curated 57 subjects.
They're all
multiple-choice questions.
They were collected just from
the web, whatever that means.
And so again, train
test overlap--
you have to be careful there.
And despite the
name, I quibble that.
It's not really about
language understanding.
It's more about
testing knowledge
because I think I'm pretty
competent in language
understanding, and I don't
think I would do that
well at MMLU because I just
don't know random facts
about foreign policy.
And the way they
evaluated at the time
the state of our
language model was
GPT-3 using few shot prompting.
So here's what the
prompt looks like.
You have a simple instruction.
You're given examples--
samples of what the format is.
Compute this.
Here's the answer.
And then the last
one is the question
with the answer choices.
And the goal is to produce the--
whatever the letter is.
This was before
instruction tuning.
So you had to really be careful.
You couldn't just say like,
answer this question zero shot.
If you gave it a
question zero shot, based
models would just ask,
generate more questions
or do something weird.
So at that time, the GPT-3
model was getting 45% accuracy.
Now I'm going to show you this.
Let's dive in a little bit
and look at these predictions.
So HELM is a framework
for evaluation
that we built that hosts a
bunch of different evaluations.
And the nice thing about
HELM is it allows you
to-- you look at
the leaderboards.
You can see how well
models are doing.
So it seems like Claude is
doing pretty well on MMLU.
And you click in, and
you can actually--
let me see the full leaderboard.
So you can see all the
different subjects and MMLU.
OK.
Let's pick one that we all
know something about, computer
science.
Good-- OK.
And if you click
through, you can actually
see all the instances.
So you have the
input, and then you
have the different
answer choices and then
what the language
model predicted
and then whether it
was correct or not.
So here's an example
of an MMLU question.
And apparently, I
guess Claude did not
get this one right and so on.
One other thing-- I think
if you dive in here,
this actually gives
you the prompt
that was fed into
the language model.
So we're doing
few-shot prompting.
So here you have the question,
answer, question, answer,
question, answer, question,
answer, question, answer.
This is five-shot and then the
final question where the answer
is meant to be filled in.
Yeah.
So I have a question here.
It seems when you were
doing future prompting,
you have 10 questions
of a similar type
of a similar topic beforehand.
Is there like any
study as to how
those questions that are
previously in your future
prompt that affect your
performance, the language
model's performance, the
question you actually ask?
Because it could be that
if it's too similar,
the initial
question, you already
answered the final question.
And the second part
of that question
is, do people still
use few-shot prompting
in evaluating [? ML ?]
new benchmark for learning
a new language?
Yeah, so the first
question is, do the choice
of few-shot examples matter?
And the answer is yes.
They definitely matter.
The order of them also matters.
The format matters because if
you happen to do classification
and you choose a bunch of
positive only positives,
then guess what?
Your language model is just
going to produce positives.
And so five examples need
to be carefully chosen.
And then the second question
is, do people still do few-shot?
Generally, it's-- I mean,
people do zero-shot,
and zero-shot have-- models have
been tuned to make zero-shot
work.
Few-shot is still
done, sometimes maybe
with one example to
essentially provide the format.
There's some bunch of papers
that analyze whether few-shot
learning is--
in-context learning is
actually learning anything
like because five
examples come out really.
Are you learning how to do US
history from five examples?
And generally, people agree
that it's more about just
telling you what the format is
and specifying what the task is.
And if you have a good
instruction volume model,
you can just write it down.
You can say, answer it
with a single letter,
and the model will do that.
So it's becoming rarer.
And also it saves
you token budget
because you don't
need to have all
these examples in your context.
So that's MMLU.
And you notice that
maybe some of you--
I don't know if anyone who
follows it maybe closely.
Like the highest numbers
are actually in the 90s,
and this is because
the prompting matters.
We use a fairly standard
prompt strategy.
But if you're doing
prompting and train
of thought and ensembling, then
you can get higher numbers.
OK, one, I guess, comment
maybe I'll make right now
is that MMLU was
started in 2020.
Remember, this is really when
there was no instruction model.
So it was meant to
evaluate base models.
And right now, it's
used to evaluate-- well,
whatever the latest models
are, which are primarily
instruction tuned.
And I think there's
this worry that,
oh, people are overfit to MMLU.
And I think that's
certainly true.
But if you look at how MMLU is,
I think, a good evaluation for--
I think of it as a good
evaluation for base models,
because if you think about
what a base model is,
you're just predicting the
next token on some corpus.
So if you were able to
magically train on a lot of data
and be able to do
well on MMLU without--
basically without even trying--
this is not studying
for the exam
and doing well on the exam--
then you probably can do--
you probably have good amount
of quote unquote "intelligence"
and can do a bunch of other
general things.
Whereas if you go and you curate
like multiple-choice questions
in the 57 subjects,
then you're probably--
you might get really
good MMU scores,
but your generality
is probably not
going to be as much as
you're estimating with MMLU.
So that's a point on
interpreting this number.
It's really a function
of not just the number,
but also what if
you're evaluating
and what the training set is.
OK, let's come back to this.
So over the years,
MMLU has been improved
by a bunch of other benchmarks.
So MMLU-Pro was this paper
that came out last year.
And they basically took MMLU.
They removed some noisy,
trivial questions.
They said, whoa, everyone's
getting 90% on MMLU.
We can't get everyone
in A, so we're
going to make it 10 choices
instead of four choices.
And the accuracy drops.
The models drop in accuracy.
I guess, by this
point, chain of thought
had been fairly common
as a way to evaluate,
which makes a lot of
sense, because if you
look at some of
the MMLU questions,
it's hard to just immediately
output the answer.
You have to think
about it for a bit.
And this is what chain
of thought gives you.
And so the whole point
was that, well, look,
MMLU-Pro scores are lower.
And I guess chain of
thought seem to help,
although not terribly
consistently.
So MMLU-Pro is, I think, you'll
see a lot of model providers--
developers kind of adopting
MMLU-Pro because you're giving--
you're not in this
saturation region that ML--
at least for
Frontier models-- is.
We can skip that.
You can click here, and you
can look at the predictions
of MMLU-Pro if you want.
Let's go on to GPQA.
So this is raising
the stakes here.
So this is actually maybe
a year or almost one,
1 and 1/2 years ago.
And here the emphasis
was explicitly
on really hard kind of
PhD-level questions,
whereas MMLU was just questions
from the internet that
could have been undergrad or
different levels, who knows.
But this was they
recruited explicitly.
People who were getting
their PhDs or had
finished their PhDs
in a particular area.
And then they had a
fairly elaborate process
for there was someone
who wrote the question,
then you get some expert to
validate it and give feedback,
and then the expert would--
basically, the question writer
would revise the question
to make it clear, and then the
expert would validate it again.
And then you give
it to a non-expert
who would spend like around 30
minutes, even without Google,
to try to answer the question.
And it turned out
that experts were
able to get 65% more or
less, and non-experts,
even with Google, can
only get like 30%.
So this is what-- their
attempt to make it
kind of really difficult. That's
why they call it Google proof.
If you search for 30
minutes on Google,
you're not going
to find the answer.
OK, so GPT-4, at the
time, got 39% accuracy.
Now let's look.
So now, this is updated.
So now, O3 is at 75.
So in the last year, there's
been quite a bit of progress
here.
I think the fact that
it's PhD or Google proof
doesn't mean that
language models can't
do a good job on this.
So one thing--
let me just like--
I don't know-- click in.
So they have this
thing where you're not
meant to put this on the web.
So we have this
little decrypt thing
that allows you have to
type in to manually view it.
So here's an example
of a question.
I'm definitely not
expert at this,
so I don't know but seems
like a question to me.
And you'll see
that actually for--
so this is O3.
Actually, the only
thing about O3
is that it basically hides
all the chain of thought.
So we don't get to look at that.
If you look at
Gemini, then I think
you can see the prediction.
So this is the question,
some biology question.
And Gemini will break down the
rationale and think for a while.
And then it says the
correct answer is D.
And it happens to be right.
Yeah.
Well, when you answered, because
the focus on this-- the focus
is Google proof,
how do you know?
How do you know?
Let's say when it's
a black box model,
like [INAUDIBLE]
or OpenAI models,
that they are not themselves
searching the web per se,
trying to find the answer?
And when you're evaluating
words with respect
to any human
benchmark, how do we
know that the human is
not using a language
model in the first place?
Like a Google benchmark may
not be a [INAUDIBLE] benchmark.
Yeah, so the question is,
is it really foolproof?
Meaning that if you
call O3, maybe O3
is secretly calling
the internet.
I mean, certainly, you have
to be careful because some
of the endpoints, they
do search the web,
but there's also a mode where
they don't search the web.
So I think we just may use the
one that doesn't search the web.
And I mean, you have to trust
that that's what's happening.
And then regarding getting
human-level accuracy,
you're saying maybe the
non-experts actually use Google
and used like O3 or something.
It's possible.
I don't know exactly how they--
I mean, I think you
just tell them not to,
and you're paying them.
I don't know.
You can monitor them.
I guess it's a little bit tricky
because now Google Gemini,
even if you're using Google,
it shows you answers.
So yeah, it's a good point.
And do you have a question?
Yeah, I was just going
to say experts also
do still achieve [INAUDIBLE].
So it's surprising.
But a lot of times, I do
wonder about [INAUDIBLE]
Yeah.
It seems like, to
me, that we're slowly
targeting more and more
expert-driven question.
It seems like we're trying
to make the models better
for a smaller and smaller
subset of the population.
Is there any research shows
that as these models get
at these more and more
like expert-level problems
that they actually also go
to the general populace?
Yeah, so the question is,
it seems like all of these
are very elite questions.
And what about the rest of
the people in the world?
We're going to see a
little bit later that--
I mean, this is only
one slice of a lecture.
There's going to
be other things.
I mean, I guess
one perspective--
I think the reason why people
focus on these type of questions
is that experts are expensive.
And so if you can
solve these tasks,
then the idea is that
if you're general, then
you can actually do
fairly complicated work.
But you're right.
I mean, there's other
things that, let's say,
responding to simple questions
or doing customer service
support, which don't
require a PhD, that
are still nonetheless valuable.
And I'll come back to
talking about how we might
address some of those issues.
OK, let me move on in
the interest of time.
So final kind of
crazy hard problem--
it's called
Humanity's Last Exam.
What a great name.
So again, there's a
lot of questions here.
This one's multimodal now.
But it's still multiple
choice, short answer.
So these are still
exam-like questions
that have a correct
answer, which is, I think,
an important limitation
because there are often
things that we ask about
which are vague and don't
have a right answer.
So this is definitely
just one subset.
And they did
something interesting.
They created a prize
pool to encourage
people to create problems,
and they offered co-authorship
to question creators.
So they got quite
a few questions,
which they used to use the
frontier language models
to reject the questions that
were, quote unquote, "too easy."
And they did a bunch of reviews.
So each of these is
like fairly time--
really, really time-consuming
to create these data sets.
And every one of these data
set graphs looks like this.
Previous benchmarks, they--
LLMs do well.
My new benchmark LLMs do poorly.
And right now, I
think HLE is up to--
I want to say like 20%.
So let's look at the latest.
Yeah, so O3 is getting 20.
So I assume this
will only just go up
with in the next--
next year, but I
know this is supposed
to be the last exam,
so I don't know what's
going to come after that.
OK, yeah.
[INAUDIBLE] I don't
know about being
able to propose a
reasonable alternative.
It's hard to-- sometimes
unfair criticism,
but the way that's designed
is almost the exact inverse
of how I would design if
I were like [INAUDIBLE]
just because if you send out
an open call for questions,
you're going to receive like
a very biased set of people
responding.
You're going to get
people who are super
exposed to LLMs already, who
know what questions are supposed
to be easy or supposed
to be difficult,
are very embedded in
the research already.
You're going to end up with the
most specific set of questions
imaginable.
It's hard to think
through, I guess.
Yeah.
Yeah, so we're basically
saying there's a huge bias here
when you're curating
or soliciting questions
because who's going to do this?
Maybe people already know LLMs,
or they have a certain thing.
Yeah, you're absolutely right.
There is definitely bias.
I think the only thing you can
say about these is that they're
hard, but they're clearly
not representative
of any particular
distribution of questions
that people are trying to ask.
Yeah, OK.
Quick question.
Actually, [INAUDIBLE]
OK, all right.
So let's talk a little bit
about instruction-following
benchmarks.
So far, all of
these have basically
been roughly multiple choice
or short answer questions.
Obviously, with
multiple choice, you
can make them as
arbitrarily hard,
and they're very structured.
So one shift that has happened
over the last four years
is the emphasis on
instruction following, which
is popularized by ChatGPT.
You just ask the model to
do stuff, and it does stuff.
So there's no notion of a
necessarily even like a task.
You just describe these new
things, new one-off tasks,
and the language
model has to do it.
So one of the main
challenges here is that,
how do you evaluate an
open-ended response in general?
And this is an unsolved problem.
And I'll show you a few
things that people do.
And each of these
has its own problems.
So Chatbot Arena, I
mentioned it before.
This is probably one of the
most popular benchmarks.
So the way it works
is that random person
from the internet
types in a prompt.
They get a response
from two models.
They don't know which the
models are coming from,
and they rate which
response is better.
And then based on these
pairwise rankings,
ELO scores are
computed, and you get
a ranking of all of the models.
So this is the current snapshot
that I just took today.
What's I think nice about
this is that these are not
static benchmarks.
It's a static prompt.
They were live kind of
coming in and dynamic.
So we are able to always
have fresh data, so to speak.
And also the ELO
rating allows you
to accommodate new models
that are coming in,
which is a feature that people
playing-- like chess players,
I guess, or figure it out.
So that's Chatbot Arena.
I don't know how many of you
saw the recent kind of scandal
around Chatbot Arena.
So over the last, I guess, two
or so years, this Chatbot Arena
has really risen in
prominence to the point where
Sundar Pichai is like tweeting
about how great Gemini is
doing on Chatbot Arena.
So it becomes a target
that mall developers are--
I mean, whatever they're
doing, they're using it for PR.
And if you know
Goodhart's law, once you
are able to measure
something, it gets hacked.
And there was this paper called
"The Leaderboard Illusion"
that talks about how there's
some providers that actually got
privileged access, or they
were able to make multiple
submissions.
There's a lot of maybe
less than ideal, I guess,
protocol for evaluation, which
hopefully will be addressed.
But so there's certainly
problems with the protocol.
There's also the
question of random people
from the internet
doing this what
distribution does that serve?
Yeah.
Random in the sense that
it could be anything?
I don't mean this
in a formal sense--
random as in whoever happens
to be going to the site.
Yeah.
So here is another
evaluation that I
think is popular called IFEval.
So the idea here is that this
is going to narrowly test
the ability of a language
model to follow constraints,
essentially.
So they come up with
a bunch of constraints
like you have to answer
with at least or at most
some number of
sentences or words.
And you have to use these words
and not these other words.
You have to format
it in a certain way.
And they basically add
these synthetic constraints
to a bunch of examples.
The nice thing is that the
constraints can be automatically
verified with just
like a simple script,
because you can just
see how many words
or how many sentences there are.
So a lot of the IF
evaluations, you
have to be very careful
because all it's doing
is evaluating whether it's
following the constraint or not.
It's not actually evaluating
the semantics of the story.
So if you generate a story
about a dog and 10 words,
it will just evaluate, did you
output a story with 10 words,
not whether the story
was good or not.
I would think about it
as a partial evaluation,
and certainly it can be gamed.
And if you look at--
maybe I don't have
time to go through it,
but the instructions are,
I would say, maybe not
the most realistic.
I would just maybe look.
So I'm planning a trip to
Japan, writing an itinerary.
You're not allowed to use
commas in your response.
OK, sure.
Or you have to use at least
12 placeholder tokens.
So I'm showing you
examples because I
think it's important
to realize what's
behind these benchmarks when
you see the numbers because most
of the people just look at
the numbers, and that's it.
So AlpacaEval is
another benchmark
where to address the
issue of, how do you
evaluate open-ended responses?
Basically, this is
computing a win rate
against a particular model as
judged by a language model.
So immediately, I know
someone's going to say, well,
this is bias.
And yes, it's biased
because you're
asking GPT-4, how much do
you like this model response
against your own generation?
But nonetheless, it
seems to be helpful.
One of the things that--
just as kind of an
interesting anecdote.
So this came out in 2023,
and then it became popular.
So a lot of people submitted
these actually smaller models
that did really well.
And it turned out that
it was gaming the system
by just having longer or longer
responses, which fooled GPT-4
into liking it.
And then so that got
corrected with this kind
of length corrected variant.
And the only thing you--
I think you can really say
here is that this is correlated
with Chatbot Arena, which means
that well, they're giving you
the same information.
This is automatic.
The other one involves humans.
So kind of I guess pick your--
if you wanted something quick
and automatic and reproducible,
AlpacaEval is a
reasonable choice.
There's kind of another
benchmark called WildBench,
which the utterances
come from a bunch
of human-bot conversations.
They put out a bot
basically for people to use,
and they collected the data
and made a data set out of it.
Again, this is using LLM as a
judge, now with a checklist,
so that it basically has
to think about the response
and make sure that it
covers certain aspects.
And this is also correlated
with a Chatbot Arena.
So interesting that
evaluation of evaluation
is correlation with
Chatbot Arena in this--
in this space.
So moving on.
Let's talk about agents a bit.
So some tasks require
tool use-- for example,
you have to run code.
You have to access
the internet, or you
have to use a calculator--
and involve iterating
over some period of time.
So if you're writing and
working on a project,
it's not an immediate thing.
You have to do it for a while.
And so this is where
agents come in.
So agents are
basically-- there's
a language model and some
of agent scaffolding, which
is basically some programmatic
logic for deciding how
the language model gets called.
And I'm going to talk
about three different agent
benchmarks, just to give you a
flavor of what that looks like.
There's SWEBench where you're
given a codebase and a GitHub
issue description.
You're supposed to
submit a PR, and the goal
is to submit the PR change
that makes the unit tests pass.
So it kind of looks like this.
Here's the issue.
And it would give the
language model the code,
and the language model
generates a patch.
And then you run the tests.
So this has been very popular
for evaluating agent benchmarks.
Here's another one
called CyBench.
And this is for
doing cybersecurity.
So the idea is that there's
these capture-the-flag
competitions where an agent
has access to a server,
and the goal is to
basically have the agent
hack into the server and
retrieve some secret key.
And if it can do that,
it solves the challenge.
To do that, the agent
essentially has to run commands.
Here's the agent
architecture, which is fairly,
I think, standard in this
space where it basically
asks the language model to
think about it and make a plan
and generate a command.
The command gets executed, and
that updates the agent's memory.
And then it iterates and does
it again and then iterates
until you either run out of
time or you've successfully
completed the task.
On these agent benchmarks, the
accuracies are still fairly low.
Now it's, I guess, up to 20%.
But the thing is that not all
the tests are created equal.
There is a first
solve time by humans.
So how long did it take a
team of humans to solve it?
The longest challenge
took 24 hours.
So now O3 is able
to solve something
that took humans 42 minutes.
So it'll be interesting to
monitor what happens here.
MLEBench is another agent
benchmark, which is interesting.
It's 75 Kaggle
competitions where
you're given a description of
the Kaggle competition and data
set, and the agent is meant
to write code, train a model,
debug, change the
hyperparameters,
and then submit.
I mean, for those of
you who've done Kaggle,
it's basically an
agent that does Kaggle.
And again, the accuracies are
in sub-20, I think, for getting,
let's say, any metal,
which is some threshold
for-- of performance.
Even the best models are
getting pretty low accuracy
at this point.
So it'll be interesting to
see what happens, I guess,
in the next year.
One thing I-- benchmark
I did want to mention
is this out in left field a bit.
All the tasks that
we've discovered
have some anchoring in
you need more knowledge.
You need linguistic knowledge.
And the question is, can
you isolate the knowledge
and factor that out and focus
exclusively on this reasoning?
And you can argue that
reasoning captures a more
pure form of intelligence.
It's not just memorizing facts.
So we want to reward models
for creativity and ability
to solve new things rather
than just I saw the internet,
and therefore I'm able
to do these tasks.
So there's something called
the ARC AGI challenge.
It was actually introduced by--
in 2019, pre-LLMs,
which is interesting.
So here's one of the tasks.
So you're given
basically these patterns,
and you're basically
trying to fill in this one.
So these are meant
to be easy for humans
to detect what the pattern is.
But there's no
there's no language,
and there's no task description.
And so the language
model actually is--
traditionally has been
pretty really bad on these.
So this is the accuracy.
And you see that
GT-40 is basically 0.
And now we're seeing kind of
O3 basically doing pretty well
on this task.
Now, per task, it's
costing quite a bit.
It's using a lot of compute--
over probably like a few
hundred dollars to do that.
But I guess if you put
that much compute into it,
you're able to do well
on this leaderboard.
And now there's an AGI-2, which
is even a harder benchmark.
So this is one class
of benchmarks that's,
I guess, in a bit
in left field, but I
think it captures something
pure about intelligence
that others aren't doing.
Bringing it back to
I guess, reality,
let me talk a little bit
about safety benchmarks.
So the idea of safety
benchmarks is-- well,
we do them in other fields.
Like we do crash testing
and safety ratings for cars,
and we do safety
for food and so on.
So what would it
mean to do it for AI?
So there's again
no clear answer.
AI is so early that I
think people haven't really
figured out what safety means,
but nonetheless, there's
a bunch of work here
maybe to mention.
There's a paper
called HarmBench,
which essentially they figure
out 510 harmful behaviors,
and they essentially prompt a
language model to see if it can
will follow the instructions.
So, for example, given
detailed instructions
for making dimethylmercury
from [INAUDIBLE] access
to a special tool, and
the correct answer is yes.
Sorry, I can't do that.
But there are
other cases where--
oops, maybe there's
no 0 on here.
OK, well, if you
look at the ratings,
I guess they're doing
reasonably well.
But some of these
models obviously
are complying and not--
like DeepSeek V3 is happily
to give you instructions.
So there's another
benchmark called AIR-Bench
where I think makes the idea
of safety a little bit more
grounded.
So they looked at different
regulatory frameworks
and company policies
and built a taxonomy
of the different types of
things that constitute safety.
So this is anchoring
safety, which
is the abstract concept and
actually law and policies
and then building a
benchmark mark around this.
And so let me just quickly
take a look at this.
So you can see that Claude
seems to be pretty reasonable,
refusing to comply with a bunch
of things, though not perfect.
And you see that some
of the other models
are maybe less good at it.
OK, one important thing I think
to discuss when you're thinking
about safety is jailbreaking.
And this is like a
meta safety thing,
because language models
are trained to refuse
harmful instructions.
But you can actually bypass
the safety if you're clever.
So there's this paper
that developed a procedure
to essentially optimize the
prompt to bypass safety.
They did it on actually open
weight model, the Llama model.
And it actually
transfers the GPT-4.
So you feed in a prompt,
which is a step-by-step plan
to destroy humanity, and
then some gibberish, which
is automatically optimized.
And then ChatGPT will
happily give you a plan.
So no, of course, I don't
think you can actually follow
this and destroy humanity.
So you could argue
that maybe this is not
the most realistic example, but
nonetheless, the fact that you
could bypass a
safety intervention
means that if there
were more, I guess,
serious high stakes issues,
then this might be a problem.
Yeah, question.
Question about the safety
net or [INAUDIBLE] refusal
rate that you were showing.
I was wondering if this is a
comprehensive-- if this also
takes into account,
for example, let's say,
the language model just like
refuses to answer anything.
Like that wouldn't
be very helpful.
Yeah.
Yeah.
Yeah.
So the question is
like-- yes, you're
absolutely right that it's easy
to be the top of the leaderboard
by just saying I don't know, or
I can't do that for everything.
So typically, you
have to pair this
with a capabilities eval that
shows that the language model--
yes, indeed, it actually does
something, and also, it's safe.
Yeah.
OK.
So a quick note about
pre-deployment testing.
The Safety Institutes
from the US and UK
and some other countries
that have established
this voluntary protocol
with model developers,
such as Anthropic and OpenAI,
where the company will give them
early access to a
model pre-release
so that they can run a
bunch of safety evaluations,
generate a report, and
then essentially give
feedback to inform
the deployment
procedure of the company.
So this is not binding.
There's no law around it.
It's just voluntary for now.
And basically, these evaluations
use some of the same evaluations
like that we've
been talking about.
But I think there's a
broader question here,
which is what exactly is safety?
And you quickly
after you-- we didn't
get a chance to really
look at all the utterances,
but you quickly realize
that a lot of safety
is strongly contextual.
They depend on the law and
politics and the social norms,
and they might vary
across country.
You might think that
safety is about refusal,
and it is at odds with
capability because the more safe
you are, the more you refuse,
and less helpful you are.
But that's not quite true
because safety is broader
than just refusal,
hallucinations,
and some of medical setting
or high stakes setting is bad.
Actually, reducing
hallucinations
makes systems more capable and
more safe, not hallucinations.
I mean, another
thing that's relevant
is there's capabilities,
and there's propensity.
So capabilities is the
ability for a language model
to do it at all.
Propensity is whether
it's been basically--
can refuse not to
do things right.
So often, the base model
will have the capabilities
and the alignment
part, which we'll
talk about in a week
or two, is the thing
that makes the language models
have less propensity to do harm.
So what you care
about, what does--
it matters.
Sorry, it depends on the regime.
So if you just
have an API model,
that only propensity
matters because you can only
access the model if it
refuses but actually knows
how to cause harm,
that's fine as long
as you can't be jailbroken.
But for open wave
models, then capability
matters as well,
because people have
shown that you can just
turn off the safety fairly
easily via fine tuning.
And to make things
more complicated,
the Safety Institute
was using CyBench
to do cybersecurity
safety because they
were worried about cyber risk.
What happens if a malicious
actor was able to use LLMs
to agents to hack into systems?
But on the other
hand, agents can
be really helpful for
doing penetration testing
before you deploy a system.
So these kind of dual
use issues make it
so that it's
actually-- capabilities
and safety are really
kind of intertwined.
So let me quickly
just go through this.
So I think a question
was brought up earlier
about realism.
So language models are used
quite a bit in practice.
But these benchmarks, especially
the standardized exam,
are pretty far away from
real world use case.
And you might think, oh,
well, as long as we get
real live traffic, we're good.
But it turns out that many
times, people are just
messing with you and doing--
giving you spammy utterances.
So that's not exactly the
distribution you want.
I think there's really
two types of prompts.
The question is, are you asking
me, or are you quizzing me?
So quizzing, the user
already knows the answer,
but it's just trying
to test the system,
and asking if the user
doesn't know the answer
is trying to get the system to
use it to use it-- to get it.
And of course, asking
prompts are more realistic
and produces value
for the user, which
means that standardized
exams, I think,
clearly are not realistic but
nonetheless can be helpful.
So there's this
paper from Anthropic
that uses language models
to analyze real world data.
So let me just show you.
So they take a bunch
of conversations,
and they use language models to
essentially hardcore cluster.
And they find basically
a distribution
over what people are
using the cloud for.
And coding is one of the
top, as you might imagine.
So one thing that's
interesting is
that once you deploy a system,
you actually have the data,
and you have the means
to actually evaluate
on realistic use cases, because
these are people paying your--
to use your API.
So they must at least care a
little bit about the response.
So there's also a project
called MedHELM where we have--
so previous medical
benchmarks were essentially
based on these
standardized exams.
Here there were 29
clinicians who were asked,
what are the real world
use cases in your practice
where language models
could be useful?
You got 121 clinical
tasks, and they
produced a bunch-- a different--
a wide suite of benchmarks
that tested for these
more realistic use cases,
such as writing up a
patient notes or planning
treatments and so on.
So this benchmark, actually,
you can see it on HELM as well.
But some of the data sets
involve patient data.
So therefore, obviously,
they're not hosted publicly.
So that's one kind of tension
that you have to deal with,
which is that realism
and privacy are at odds.
So let's talk about
validity here.
So train-test overlap, that five
minutes of a lecture and someone
asked about that.
So you know not to
train on your test set.
And previously, we didn't have
to think very much about this
because some benchmark
designer carefully divided
train and test.
And nowadays, people
train internets,
and they don't tell you
about what their data is.
So this is basically impossible.
Route one, what you can
do is you can be clever,
and you try to infer whether
your test set was trained on
by trying to query the model.
There's some kind of
interesting tricks
that you can use by noticing
that if the language
model prescribes a
certain type of order,
it favors a certain
type of order
that correlates with
a data set order then.
That's a sign that
it's been trained on.
Route 2 is that you
can encourage norms.
So there's this paper that
essentially looked at,
how often was it the case that
when someone, a model provider,
reported a data
set they actually
tested, whether their test set
was not in the training set?
And providers definitely do, but
it's definitely not the norm.
So you can think about
this as akin to well, you
report numbers, and you should
report whether confidence
intervals or standard errors.
And maybe this is something
that the community
can work on improving.
There's also issues
of data set quality.
SWEBench apparently had
some errors that got fixed.
Many benchmarks
actually have errors.
So if you see these scores
like math and [? GSMK, ?]
they're at like 90-plus percent.
And you wonder, well, man, those
questions must be really hard.
And it turns out
that half of them
are actually just
noise, label noise.
So once they get fixed,
then the numbers go up.
OK, final comment.
So what are we even evaluating?
Before, we were
evaluating methods.
Because you fix, train, test.
You have a new architecture.
You do a new learning algorithm.
You train, and then you test.
And you get some
number that tells you
how good your method is.
Today, I think it's an important
distinction that we're not
evaluating methods.
We're evaluating systems
where anything goes.
And there's some exceptions.
So not all GPT is a
speed run competition
where given a fixed data set and
you basically minimize the time
to get to a particular
loss, and datacomp,
which you're trying
to select data
to get you a level of accuracy.
And these are
helpful for getting--
encouraging algorithmic
innovation from researchers.
But evaluating systems is
also really useful for users.
So again, I think it's important
to define the rules of the game
and also to think about what is
the purpose of your evaluation.
So hopefully, that
was a whirlwind tour
of different aspects
of evaluation.
Hope that was interesting.
OK, that's all.
See you next time.